{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0d00-7f18-4047-83e9-263316278bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pyts.image import MarkovTransitionField\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1507f595-2818-4106-bfa8-6c2b85d1668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for graph creation\n",
    "def create_graph(main_path, path_properties = None, path_mask = None, classif_type = 0):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if path_properties == None and path_mask == None: #\n",
    "    \n",
    "        df = pd.read_csv(main_path)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    else:\n",
    "        dataset_rss = np.load(main_path, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss * (-1)\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    #output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    #setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    #function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]#adj_mat[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels\n",
    "        \n",
    "        if classif_type == 0: ##for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        else:                 ##for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b197a-136c-4297-a08a-9994bf305136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a01a6d5-aa80-4aae-b121-6486c5e5f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition for graph classification\n",
    "class GINE(torch.nn.Module):\n",
    "    \"\"\"GIN\"\"\"\n",
    "    def __init__(self, dim_h):\n",
    "        super(GINE, self).__init__()\n",
    "        edge_dim = 1\n",
    "        \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ca21f0-8b61-4394-9d6e-d75edf1b7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training the model\n",
    "def trainG(model, loader, epoch, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "         for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            \n",
    "            #define loss_function\n",
    "            loss_function = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "            loss = loss_function(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "#function for testing the model\n",
    "def testG(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    predicted_categories = []\n",
    "    true_categories = []\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "        #Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data)  \n",
    "            \n",
    "            #Use the class with highest probability.\n",
    "            pred = out.argmax(dim=1)\n",
    "            predicted_categories.append(pred.cpu().detach().numpy())\n",
    "            true_categories.append(data.y.cpu().detach().numpy())\n",
    "            \n",
    "            #Check against ground-truth labels.\n",
    "            correct += int((pred == data.y).sum())\n",
    "    print(confusion_matrix(true_categories, predicted_categories))\n",
    "    print(classification_report(true_categories, predicted_categories))  \n",
    "    \n",
    "    #Derive ratio of correct predictions.\n",
    "    return correct / len(loader.dataset)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f4ea5-9707-4b70-9426-8c2d77abda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_classification(main_path, path_properties = None, path_mask = None, range_epoch = 10):\n",
    "    \n",
    "    #calling the graph creator function\n",
    "    output = create_graph(main_path, path_properties, path_mask)\n",
    "    torch.manual_seed(6406)\n",
    "    \n",
    "    #setting train and test sizes\n",
    "    train_size = int(0.8 * len(output))\n",
    "    test_size = len(output) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, test_size])\n",
    "    loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "    \n",
    "    #setting device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #creating model\n",
    "    model = GINE(32).double().to(device)\n",
    "    \n",
    "    #selecting the optimiser\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    #run for number of epochs\n",
    "    for epoch in range(range_epoch):\n",
    "        result = trainG(model, loader, epoch, optimizer, device)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    score = testG(model, DataLoader(test_dataset, batch_size = 1), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02438ed-0f1a-4a69-b53e-4971b2906b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Node Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c969f63-68b7-49c7-8470-b9511539bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(1, 4, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(1, 16)\n",
    "        self.conv2 = GATConv(16, 4, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(16, 16)\n",
    "        self.conv3 = GATConv(16, 1, heads=6,concat=False)\n",
    "        self.lin3 = torch.nn.Linear(16, 1)\n",
    "          \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fcb3ba-bfeb-4434-96e9-85acac01eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training and validating the model\n",
    "def trainN(model, epoch, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x.float(), data.edge_index), data.y)\n",
    "        total_loss += loss.item() * len(train_loader)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "#function for testing the model\n",
    "def testN(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        out = model(data.x.float().to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())  \n",
    "        \n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0, ys,preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6c81d50-66af-4752-87b6-0abf1fd032cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_classification(main_path, path_properties = None, path_mask = None, range_epoch = 10):\n",
    "    \n",
    "    #calling the graph creator function\n",
    "    output = create_graph(main_path, path_properties, path_mask, classif_type = 1)\n",
    "    torch.manual_seed(6406)\n",
    "    \n",
    "    #setting train, val and test sizes\n",
    "    train_size = int(0.6 * len(output))\n",
    "    Temp_size = len(output) - train_size\n",
    "    val_size = int(0.5*Temp_size)\n",
    "    test_size = Temp_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "    global train_loader, val_loader, test_loader, loss_op\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    #setting device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #creating model\n",
    "    model = Net().to(device)\n",
    "    #define loss\n",
    "    loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    #run for number of epochs\n",
    "    for epoch in range(range_epoch):\n",
    "        loss = trainN(model, epoch, optimizer, device)\n",
    "        val_f1,ys,preds = testN(model, val_loader, device)\n",
    "        test_f1,ys,preds = testN(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_f1:.4f}, 'f'Test: {test_f1:.4f}')\n",
    "        \n",
    "    print(sklearn.metrics.multilabel_confusion_matrix(ys[0], preds[0]))\n",
    "    print(sklearn.metrics.classification_report(ys[0], preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36e3bd-7260-4dab-b63a-6417dc2cdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pypeline(classif, main_path, path_properties = None, path_mask = None, range_epoch = 10):\n",
    "    if classif == \"graph\"\n",
    "        node_classification(main_path, path_properties, path_mask, range_epoch = 10)\n",
    "    \n",
    "    if classif == \"node\"\n",
    "        graph_classification(main_path, path_properties, path_mask, range_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3df02b6-b917-4e1c-8ba8-1028a2209d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you running all functions above you have one main functions to call:\n",
    "\n",
    "#     - run_pypeline(classif == either \"graph\" or \"node\", what kind of classification you want,\n",
    "#                    main_path == .csv, path of the cut/uncut dataset,\n",
    "#                    path_properties == .npz, fill this, if you want the cut dataset, with properties, otherwies leave blank for uncut,\n",
    "#                    path_mask ==  .npz , fill this, if you want the cut dataset, with mask, otherwies leave blank for uncut,\n",
    "#                    range_epoch == 10, for length of epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c694bd-1f81-45ce-9ba2-2178a58d6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pypeline(classif = \"node, \"main_path = \"dataset_cut.csv\", range_epoch = 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
