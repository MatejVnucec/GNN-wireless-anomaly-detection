{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172cdea-ff23-453a-b0a7-c5298be3f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install mkl=2019.1.* -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0d00-7f18-4047-83e9-263316278bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch=1.12 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html #==2.1.0+pt112cu113\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html #==0.6.16+pt112cu113\n",
    "!pip install torch-geometric #==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y #==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython#==0.29.24\n",
    "!pip install numba#==0.56.2\n",
    "!pip install pandas#==1.3.4\n",
    "!pip install networkx#==2.6.3\n",
    "!pip install matplotlib#==3.4.3\n",
    "!pip install ts2vg#==1.0.0\n",
    "!pip install pytorch_lightning==1.6.5\n",
    "!pip install tensorflow#==2.11.0\n",
    "!pip install dvclive#==1.3.3\n",
    "# !pip install torchsummary\n",
    "!pip install torchmetrics==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a6b9e-1cf9-4689-9a01-e5e9deaf54d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# nyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1a450-11a7-4ec4-98e6-dbe6ce1b59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch-cuda=11.6 -c pytorch -c conda-forge -c nvidia -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5a746-4652-410e-be40-4119b0ebc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d93a4-04f9-4c6d-84f6-dac52e9a93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.1+cu116.html -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac34cc-78c1-4c7f-bc70-5a6a64fd4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ts2vg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3e69b-a881-4d08-8cc1-0a9ce27ee5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning==1.6.5 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278f62d-a38b-4a14-acde-04211ac02f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mamba install tensorflow-gpu -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c677c-7da5-4c62-aa0e-f04ca28860c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y #==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c815cd3-bca0-494b-836d-23681c4870c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffd9a2-c39a-42b0-be11-ff34e48193b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import ts2vg\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pyts.image import MarkovTransitionField\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "#from pytorch_lightning.callbacks import LearningRateFinder\n",
    "#from pytorch_lightning.callbacks import BatchSizeFinder\n",
    "from pytorch_lightning.callbacks import ProgressBarBase\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1de229-0f4d-4a14-8b20-711d31b181c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in Y[2123*3:2123*4]:\n",
    "    if i == 3:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e71d6e-92ae-4788-87d7-de90a660273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        graph_type = \"vis\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "        classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "        len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "\n",
    "        path_main = \"datasets/dataset_50_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "        path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "        path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "        learning_rate = 0.01\n",
    "        batch_size = 64\n",
    "        range_epoch = 1000 #set length of epoch\n",
    "        save_file=\"Models\"\n",
    "        name_of_save = \"VG_node_classification_rounded\"\n",
    "        patience = 500\n",
    "        analysis = True\n",
    "        \n",
    "        # df = pd.read_csv(path_main)  \n",
    "        # del df['Unnamed: 0']\n",
    "        # df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        # length_rss = int((df.columns.stop-2)/2)\n",
    "        # global X_mask\n",
    "        # X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        # #X = np.round_(X,1)\n",
    "        # Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        # X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = torch.from_numpy(df.loc[:,df.columns[:length_rss]].values) # x values for every sample\n",
    "        #X = np.round_(X,1)\n",
    "        Y = torch.from_numpy(df[length_rss+1].values).type(torch.long) # types of anomalies\n",
    "        X_mask = torch.from_numpy(df.loc[:,df.columns[length_rss+2:]].values) # binary location of anomalies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eceb5a-cd66-46f5-aa26-a4d68ebca85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[]\n",
    "for i, val in enumerate(Z):\n",
    "    if val == -87.63:\n",
    "        print(i)\n",
    "        W.append(val)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d2ab2-0b6f-4077-a304-ef8a142502f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.zeros([50*5], dtype='float')\n",
    "KM = np.zeros([50*5], dtype='float')\n",
    "KA = np.zeros([50*5], dtype='float')\n",
    "\n",
    "K[:50]=(X[(2123*0)+744])\n",
    "K[200:]=(X[(2123*1)+1368])\n",
    "K[50:100]=(X[(2123*2)+1699])\n",
    "K[100:150]=(X[(2123*3)+1842])\n",
    "K[150:200]=(X[(2123*4)+1864])\n",
    "\n",
    "KM[:50]=(X[(2123*0)+744])\n",
    "KM[200:]=(X[(2123*0)+1368])\n",
    "KM[50:100]=(X[(2123*0)+1699])\n",
    "KM[100:150]=(X[(2123*0)+1842])\n",
    "KM[150:200]=(X[(2123*0)+1864])\n",
    "\n",
    "KA[:50]=(X_mask[(2123*0)+744])\n",
    "KA[200:]=(X_mask[(2123*1)+1368])\n",
    "KA[50:100]=(X_mask[(2123*2)+1699])\n",
    "KA[100:150]=(X_mask[(2123*3)+1842])\n",
    "KA[150:200]=(X_mask[(2123*4)+1864])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d7090-e1ab-4610-a74b-31f65ec6e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ji=0\n",
    "temp=[]\n",
    "for i in range(len(KA)):\n",
    "    if ji != KA[i]:\n",
    "        temp.append(i)\n",
    "        if ji == 0:\n",
    "            ji = 1\n",
    "        else:\n",
    "            ji =0\n",
    "ji = temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc75d4-4cf1-4acf-9da2-7f9e40ddc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "Y_index= list(range(0, 250))\n",
    "figure(dpi=200, figsize=(9, 4))\n",
    "k = 0.5\n",
    "g = 1\n",
    "const = 'black'\n",
    "plt.plot(KM, color=const, linewidth=0.3,linestyle='dotted')\n",
    "plt.plot(Y_index[:ji[0]],K[:ji[0]], color=const, linewidth=k, label = 'No anomaly')\n",
    "plt.plot(Y_index[ji[0]-1:ji[1]+1],K[ji[0]-1:ji[1]+1], color='red', linewidth=g, label='SuddenR anomaly')\n",
    "plt.plot(Y_index[ji[1]:ji[2]],K[ji[1]:ji[2]], color=const, linewidth=k)\n",
    "plt.plot(Y_index[ji[2]-1:ji[3]+1],K[ji[2]-1:ji[3]+1], color='blue', linewidth=g, label='InstaD anomaly')\n",
    "plt.plot(Y_index[ji[3]:ji[4]],K[ji[3]:ji[4]], color=const, linewidth=k)\n",
    "plt.plot(Y_index[ji[4]-1:ji[5]+1],K[ji[4]-1:ji[5]+1], color='blue', linewidth=g)\n",
    "# plt.plot(Y_index[ji[5]:ji[6]],K[ji[5]:ji[6]], color=const, linewidth=k)\n",
    "# plt.plot(Y_index[ji[6]-1:ji[7]+1],K[ji[6]-1:ji[7]+1], color='blue', linewidth=g)\n",
    "# plt.plot(Y_index[ji[7]:ji[8]],K[ji[7]:ji[8]], color=const, linewidth=k)\n",
    "# plt.plot(Y_index[ji[8]-1:ji[9]+1],K[ji[8]-1:ji[9]+1], color='blue', linewidth=g)\n",
    "plt.plot(Y_index[ji[5]:ji[6]+1],K[ji[5]:ji[6]+1], color=const, linewidth=k)\n",
    "plt.plot(Y_index[ji[6]-1:ji[7]+1],K[ji[6]-1:ji[7]+1], color='green', linewidth=g, label='SlowD anomaly')\n",
    "plt.plot(Y_index[ji[7]:ji[8]+1],K[ji[7]:ji[8]+1], color=const, linewidth=k)\n",
    "plt.plot(Y_index[ji[8]-1:],K[ji[8]-1:], color='orange', linewidth=g, label='SuddenD anomaly')\n",
    "\n",
    "plt.xlabel('packet number (0.1 sec/packet)')\n",
    "plt.ylabel('RSS (dBm)')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('start_image.eps', format='eps')\n",
    "plt.savefig('start_image.png', format='png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10178746-ca3d-40dc-aa7f-e68d528b0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([2, 4, 6, 8, 10, 12])\n",
    "mask = np.array([1, 0, 1, 0, 1, 0])\n",
    "\n",
    "mask_indices = np.where(mask == 1)[0]\n",
    "\n",
    "plt.plot(x[mask_indices], y[mask_indices], 'ro')\n",
    "\n",
    "mask_indices = np.where(mask == 0)[0]\n",
    "\n",
    "plt.plot(x[mask_indices], y[mask_indices], 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0732451-077d-40d7-a6aa-6a293ad2076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        k = 8015\n",
    "        g = NaturalVG(weighted='abs_v_distance')\n",
    "        #g = HorizontalVG(weighted='h_distance')\n",
    "        \n",
    "        g.build(X[k])\n",
    "\n",
    "        adj_mat_vis = np.zeros([len(X[k]),len(X[k])], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q = g.edges[i]\n",
    "            adj_mat_vis[x,y] = q\n",
    "            #adj_mat_vis[y,x] = q\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_vis).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_vis[row, col]\n",
    "        plt.imshow(adj_mat_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299550c8-7d6a-46d5-908c-a6d62c751153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_type = \"dual_VG\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "vis_type = \"horizontal\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'abs_v_distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "\n",
    "def vis_matrix(X_current, vis_type_local, dual = False):\n",
    "    if vis_type_local == \"natural\":\n",
    "        g = NaturalVG(weighted='abs_v_distance')\n",
    "    elif vis_type_local == \"horizontal\":\n",
    "        g = HorizontalVG(weighted='abs_v_distance')\n",
    "\n",
    "    g.build(X_current)\n",
    "\n",
    "    adj_mat_vis = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "    for i in range(len(g.edges)):\n",
    "        x, y, q =g.edges[i]\n",
    "        adj_mat_vis[x,y] = q\n",
    "        if vis_edge_type == \"undirected\":\n",
    "            adj_mat_vis[y,x] = q\n",
    "    \n",
    "    if dual == True and vis_edge_type == \"directed\":\n",
    "        adj_mat_vis =  adj_mat_vis + vis_matrix(-X_current,\"natural\", vis_type).T\n",
    "    \n",
    "    return adj_mat_vis\n",
    "\n",
    "pos_adj_mat_vis = vis_matrix(X[8001], \"natural\",dual = True)\n",
    "# neg_adj_mat_vis = vis_matrix(-X[8001], \"natural\",\"directed\")\n",
    "\n",
    "edge_index = torch.from_numpy(pos_adj_mat_vis).nonzero().t().contiguous()\n",
    "\n",
    "#join two edge_weight arrays\n",
    "row, col = edge_index\n",
    "# edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "edge_weight = pos_adj_mat_vis[row, col]\n",
    "# edge_weight[:,1] = neg_adj_mat_vis[row, col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c6847-8e36-417d-8e3e-47832a2d87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pos_adj_mat_vis + neg_adj_mat_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247abbf9-bac7-47bf-a3bd-c9055723151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7d87c-175d-4614-ae54-057b19999919",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(neg_adj_mat_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b7d43-dab5-4b87-b53e-9107a20d3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {\n",
    "\n",
    "        \"Normal\" : 0,\n",
    "        \"Vertical\" : 100000,  \n",
    "        \"AbsVertical\" : 200000,  \n",
    "        \"MTF\" : 0,  \n",
    "        \"VG\" : 1000, \n",
    "        \"MTF_on_VG\" : 2000,  \n",
    "        \"VG_on_MTF\" : 3000,  \n",
    "        \"HVG\" : 4000,  \n",
    "        \"VG_dir\" : 5000,\n",
    "        \"graph\" : 0,  \n",
    "        \"node\" : 100,  \n",
    "        \"uncut\" : 0,  \n",
    "        \"random\" : 10,  \n",
    "\n",
    "        \"50\" : 20,  \n",
    "        \"100\" : 30,  \n",
    "        \"150\" : 40,  \n",
    "        \"200\" : 50,  \n",
    "        \"250\" : 60,  \n",
    "        \"25\" : 70\n",
    "}\n",
    "Dict[\"Normal\"] + Dict[\"VG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e29b5-a03e-4828-be94-44fded8b9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (-71,-69,-90,-89,-67,-69,-68,-69)\n",
    "#g = NaturalVG(weighted='distance')\n",
    "g = HorizontalVG(weighted='distance', directed = None) # left_to_right # top_to_bottom\n",
    "\n",
    "g.build(X)\n",
    "\n",
    "adj_mat_vis = np.zeros([len(X),len(X)], dtype='float')\n",
    "for i in range(len(g.edges)):\n",
    "    x, y, q = g.edges[i]\n",
    "    adj_mat_vis[x,y] = q\n",
    "    adj_mat_vis[y,x] = q\n",
    "\n",
    "edge_index = torch.from_numpy(adj_mat_vis).nonzero().t().contiguous()\n",
    "row, col = edge_index\n",
    "edge_weight = adj_mat_vis[row, col]\n",
    "plt.imshow(adj_mat_vis)\n",
    "plt.savefig('adj_matrix_plot.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452240e-a03d-4c1d-bc77-30cddb11b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd9171-bd4a-4ca4-a104-37eb2c444349",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf08e3-f893-4193-b1b5-95af5452dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b547bf8-b571-4d6b-8370-2c4f4064a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adj_mat_vis)\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"RSSI\")\n",
    "# plt.savefig('adj_matrix_plot.eps', format='eps')\n",
    "# plt.savefig('adj_matrix_plot.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b1b52-626e-4eb9-886a-e6f050c23778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "in this bracket of code is located the conversion of time series to different graph neural networks (Markov transition field (MTF) and Visibility graphs (VG)) \n",
    "with different lengths (uncut(300), cut and random(where every sample has a different length between 300 and 50)).\n",
    "\n",
    "create graph is called when we have a time series that needs to be converted into a graph. I\n",
    "\n",
    "\"\"\"\n",
    "def create_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy() # x values for every sample\n",
    "        #X = np.round_(X,1)\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8) # types of anomalies\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy() # binary location of anomalies\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "            MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "            X_gaf_MTF = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            dataset_properties[i,1] = int(dataset_properties[i,1])\n",
    "        \n",
    "        X = dataset_rss # x values for every sample\n",
    "        X_mask = dataset_mask # binary location of anomalies\n",
    "        Y = dataset_properties[:,2] # types of anomalies\n",
    "        Y_len = dataset_properties[:,0] # length of every sample\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "            X_gaf_MTF = []\n",
    "            for i in range(len(Y_len)):\n",
    "                MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "                X_gaf_MTF_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "                X_gaf_MTF.append(X_gaf_MTF_temp[0])\n",
    "    \n",
    "    # output will contain all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    def vis_matrix(X_current, vis_type_local):\n",
    "        if vis_type_local == \"natural\":\n",
    "            g = NaturalVG(weighted=vis_distance)\n",
    "        elif vis_type_local == \"horizontal\":\n",
    "            g = HorizontalVG(weighted=vis_distance)\n",
    "            \n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_vis = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_vis[x,y] = q\n",
    "            if vis_edge_type == \"undirected\":\n",
    "                adj_mat_vis[y,x] = q\n",
    "        \n",
    "        return adj_mat_vis\n",
    "    \n",
    "    # functions for creating edge index and edge weight for a given matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        #function for visibility and MTF matrixes\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]\n",
    "        return edge_index,edge_weight\n",
    "    \n",
    "    def adjToEdgidx_join(adj_mat_MTF,X_current):\n",
    "        #function for joined visibility and MTF matrixes\n",
    "        adj_mat_vis = vis_matrix(X_current, vis_type)\n",
    "            \n",
    "        if graph_type == \"vis_on_MTF\":\n",
    "            temp_main_adj_mat = adj_mat_MTF\n",
    "        elif graph_type == \"MTF_on_vis\":\n",
    "            temp_main_adj_mat = adj_mat_vis\n",
    "            \n",
    "        edge_index = torch.from_numpy(temp_main_adj_mat).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visibility is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_vis[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def adjToEdgidx_dual_VG(X_current):\n",
    "        #function for joined visibility and MTF matrixes\n",
    "        pos_adj_mat_vis = vis_matrix(X_current, vis_type)\n",
    "        neg_adj_mat_vis = vis_matrix(-X_current, vis_type)\n",
    "            \n",
    "        edge_index = torch.from_numpy(pos_adj_mat_vis+neg_adj_mat_vis).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays\n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = pos_adj_mat_vis[row, col]\n",
    "        edge_weight[:,1] = neg_adj_mat_vis[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def adjToEdgidx_double_VG(X_current):\n",
    "        \n",
    "        adj_mat_vis1 = vis_matrix(X_current, \"Natural\")\n",
    "        adj_mat_vis2 = vis_matrix(X_current, \"Horizontal\")\n",
    "            \n",
    "        edge_index = torch.from_numpy(adj_mat_vis1).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visibility is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_vis1[row, col]\n",
    "        edge_weight[:,1] = adj_mat_vis2[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def define_mask(i):\n",
    "        if classif == \"graph\": # for graph classification\n",
    "            return torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":# for node classification \n",
    "            return torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "    if graph_type == \"MTF\":  \n",
    "        for i, j in enumerate(X_gaf_MTF):\n",
    "            print(i)\n",
    "            edge_index, edge_weight = adjToEdgidx(j)\n",
    "            y_mask = define_mask(i)\n",
    "            #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally label\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    elif graph_type == \"vis\":\n",
    "        for i in range(len(X)):\n",
    "            print(i)\n",
    "            edge_index, edge_weight = adjToEdgidx(vis_matrix(X[i],vis_type))\n",
    "            y_mask = define_mask(i)\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=torch.tensor(edge_index, dtype=torch.int64), edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),y=y_mask))    \n",
    "    \n",
    "    elif graph_type in (\"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "        for i, j in enumerate(X_gaf_MTF):\n",
    "            print(i)\n",
    "            edge_index, edge_weight = adjToEdgidx_join(j, \n",
    "                                                       vis_matrix(X[i],vis_type))\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    elif graph_type == \"double_VG\":\n",
    "        for i in range(len(X)):\n",
    "            print(i)\n",
    "            edge_index, edge_weight = adjToEdgidx_double_VG(X[i])\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    \n",
    "    elif graph_type == \"dual_VG\":\n",
    "        for i in range(len(X)):\n",
    "            print(i)\n",
    "            edge_index, edge_weight = adjToEdgidx_dual_VG(X[i])\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    \n",
    "    return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb08e8-42df-43ce-b6c3-b061b3077bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the last output so we dont have to generate another if all parameters are the same as before\n",
    "global temp_repeat\n",
    "temp_repeat=['']*9\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask or temp_repeat[6] != vis_type or temp_repeat[7] != vis_distance or temp_repeat[8] != vis_edge_type:\n",
    "    \n",
    "        global output\n",
    "        output = create_graph()\n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    temp_repeat[6] = vis_type\n",
    "    temp_repeat[7] = vis_distance\n",
    "    temp_repeat[8] = vis_edge_type\n",
    "    \n",
    "    print(temp_repeat[1] + \" classification on \" + temp_repeat[2] + \" time series using \" + temp_repeat[0] + \" graphs\" )\n",
    "    return output\n",
    "\n",
    "def check_for_missclick():\n",
    "    return graph_type in (\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\",\"double_VG\",\"dual_VG\") and classif in (\"graph\", \"node\") and len_type in (\"un/cut\", \"random\") and vis_type in (\"natural\", \"horizontal\") and vis_distance in ('slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance') and vis_edge_type in (\"undirected\", \"directed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37303612-ad33-4149-bb9c-f5c62aa27d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "#     def __init__(self, milestones, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.milestones = milestones\n",
    "\n",
    "#     def on_fit_start(self, *args, **kwargs):\n",
    "#         return\n",
    "\n",
    "#     def on_train_epoch_start(self, trainer, pl_module):\n",
    "#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "#             self.lr_find(trainer, pl_module)\n",
    "            \n",
    "# class LitProgressBar(ProgressBarBase):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__() \n",
    "#         self.enable = True\n",
    "\n",
    "#     def disable(self):\n",
    "#         self.enable = False\n",
    "\n",
    "#     def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):\n",
    "#         super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)\n",
    "#         percent = (self.train_batch_idx / self.total_train_batches) * 100\n",
    "#         sys.stdout.flush()\n",
    "#         sys.stdout.write(f'{percent:.01f} percent complete \\r')\n",
    "\n",
    "# lr_finder = FineTuneLearningRateFinder(milestones=(5,10))\n",
    "            \n",
    "class GINE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(GINE, self).__init__()\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis\"):\n",
    "            edge_dim = 1\n",
    "        elif graph_type in (\"MTF_on_vis\", \"vis_on_MTF\",\"double_VG\",\"dual_VG\"):\n",
    "            edge_dim = 2\n",
    "            \n",
    "        dim_h = 32\n",
    "    \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "                     \n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device) #weight=class_weights\n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        correct=out.argmax(dim=1).eq(val_batch.y).sum().item()\n",
    "        total=len(val_batch.y)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        \n",
    "        logs={\"train_loss\": val_loss}\n",
    "        batch_dictionary={\"loss\": val_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_true\", test_label)\n",
    "        self.log(\"test_pred\", pred)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, test_label\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        true_array=[]\n",
    "        pred_array = []\n",
    "        for i in range(len(outputs)):\n",
    "            true_array = np.append(true_array,outputs[i][1].cpu().numpy())\n",
    "            pred_array = np.append(pred_array,outputs[i][0].cpu().numpy())            \n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array, digits=3))\n",
    "    \n",
    "#---------------------------------------------------------------------------------\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(1, 32, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(1, 4 * 32)\n",
    "        self.conv2 = GATConv(4 * 32, 32, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 32, 4 * 32)\n",
    "        self.conv3 = GATConv(4 * 32, 1, heads=6,concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):        \n",
    "        out = model(train_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device) #weight=class_weights[1]\n",
    "        \n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        val_label = val_batch.y.cpu()\n",
    "        ys.append(val_batch.y)\n",
    "        preds.append((out > 0).float().cpu())     \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "    \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        test_label = test_batch.y.cpu()\n",
    "        ys.append(test_batch.y)\n",
    "        preds.append((out > 0).float().cpu())\n",
    "        \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        \n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, y\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        global true_array, pred_array\n",
    "        true_array=[]\n",
    "        pred_array = []\n",
    "        for i in range(len(outputs)):\n",
    "            true_array = np.append(true_array,outputs[i][1].cpu().numpy())\n",
    "            pred_array = np.append(pred_array,outputs[i][0].cpu().numpy())   \n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array, digits=3))\n",
    "        print(\"pred_array \",pred_array)\n",
    "        pd.DataFrame(pred_array).to_csv(\"pred_array.csv\")\n",
    "        pd.DataFrame(true_array).to_csv(\"true_array.csv\")\n",
    "\n",
    "        \n",
    "def main():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        global model, test_loader, device\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_acc',patience=150, strict=False,verbose=False, mode='max')\n",
    "        model_chackpoint = ModelCheckpoint(monitor='val_loss', mode='min')\n",
    "        val_checkpoint_acc = ModelCheckpoint(filename=\"max_acc-{epoch}-{step}-{val_acc:.3f}\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_best = ModelCheckpoint(filename=\"best\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_loss = ModelCheckpoint(filename=\"min_loss-{epoch}-{step}-{val_loss:.3f}\", monitor = \"val_loss\", mode=\"min\")\n",
    "        latest_checkpoint = ModelCheckpoint(filename=\"latest-{epoch}-{step}\", monitor = \"step\", mode=\"max\",every_n_train_steps = 500,save_top_k = 1)\n",
    "        batchsizefinder = BatchSizeFinder(mode='power', steps_per_trial=3, init_val=2, max_trials=25, batch_arg_name='batch_size')\n",
    "        \n",
    "        torch.manual_seed(SEED)\n",
    "        print(SEED)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        output = generate_output()\n",
    "        # my data        \n",
    "        \n",
    "        train_size = int(0.8 * len(output))\n",
    "        Temp_size = len(output) - train_size\n",
    "        val_size = int(0.2*Temp_size)\n",
    "        test_size = Temp_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "                \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # mode\n",
    "        if classif == \"graph\":\n",
    "            model = GINE().double()#.to(device) #GINE is for graph classification\n",
    "        elif classif == \"node\":\n",
    "            model = Net().double()#.to(device) #net is for node classification\n",
    "            \n",
    "        #training\n",
    "        logger = TensorBoardLogger(save_file, name=name_of_save) # where the model saves the callbacks\n",
    "        val_check_interval=len(train_loader)\n",
    "        bar = LitProgressBar()\n",
    "        \n",
    "        trainer = pl.Trainer(logger=logger,max_epochs = range_epoch, callbacks=[latest_checkpoint, val_checkpoint_acc,val_checkpoint_loss,val_checkpoint_best,early_stop],accelerator='gpu',devices=1)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "def result_showcase():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        global model, test_loader, device\n",
    "    \n",
    "        import os, sys\n",
    "        versions = os.listdir(name_of_save + \"/\"+version)\n",
    "        len_ver = len(versions)\n",
    "        SEED_temp=SEED\n",
    "        for i in range(len_ver):#len_ver\n",
    "\n",
    "            global device, model, train_loader,val_loader, test_loader\n",
    "\n",
    "            torch.manual_seed(SEED_temp)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            output = generate_output()\n",
    "            train_size = int(0.8 * len(output))\n",
    "            Temp_size = len(output) - train_size\n",
    "            val_size = int(0.2*Temp_size)\n",
    "            test_size = Temp_size - val_size\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "                        # mode\n",
    "            if classif == \"graph\":\n",
    "                model = GINE().load_from_checkpoint(name_of_save + \"/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double() # \"DvcLiveLogger/dvclive_run/checkpoints\"+ monitor_test\n",
    "            elif classif == \"node\":\n",
    "                model = Net.load_from_checkpoint(name_of_save+\"/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double()\n",
    "            trainer = pl.Trainer(accelerator='gpu',devices=1)\n",
    "            trainer.test(model, test_loader)\n",
    "            print(\"version_\"+str(i))\n",
    "            print(SEED_temp)\n",
    "            SEED_temp += 1\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "#main parametrs\n",
    "\n",
    "graph_type = \"MTF\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"random\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "#paths\n",
    "path_main = \"datasets/dataset_rss.npz\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "# params for \n",
    "SEED = 10\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "learning_rate = 0.01\n",
    "batch_size = 64*2\n",
    "range_epoch = 500 #set length of epoch\n",
    "save_file=\"tb_logs\"\n",
    "name_of_save = \"test\"\n",
    "analysis = True\n",
    "#-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c01a1-0391-4f3f-a958-b442fa942a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"vis_1206000\"\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "\n",
    "graph_type = \"vis\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "vis_type = \"natural\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'abs_v_distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties_50_300.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask_50_300.npz\" # path to mask dataset used for random\n",
    "SEED = 205000\n",
    "\n",
    "result_showcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8e2fd-cf27-4de2-918c-fa39604673f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"HVG_dual_v_abs_209000\"\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "\n",
    "graph_type = \"dual_VG\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "vis_type = \"horizontal\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'abs_v_distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d814b73-0317-42fd-81e3-147ee4e778ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"HVG_dual_v_abs_209000\"\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "\n",
    "graph_type = \"dual_VG\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "vis_type = \"horizontal\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'abs_v_distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties_50_300.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask_50_300.npz\" # path to mask dataset used for random\n",
    "SEED = 209000\n",
    "\n",
    "result_showcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc249907-05f0-4429-99dc-ca8860256095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_type(my_model):\n",
    "    if len(my_model) == 300:\n",
    "        my_model[0] = 0\n",
    "        my_model[1] = 0\n",
    "        my_model[2] = 0\n",
    "\n",
    "    if len(my_model)%2 != 0\n",
    "        my_model = np.append(0,my_model)\n",
    "    par1 = int(len(my_model)/2)\n",
    "    \n",
    "    #check if anywhere in array there is an anomaly, otherwise predict this is a No_anomaly\n",
    "    if any(my_model) == 1:\n",
    "        \n",
    "        mm1 = [all(i) for i in my_model.reshape(par1, -1)]\n",
    "        mm2 = [all(i) for i in my_model[1:-1].reshape(par1, -1)]\n",
    "        #check if anywhere in the array there are two 1 gruped together, otherwise predict this is a InstaD\n",
    "        if any(mm1) == 1 and any(mm2) == 1:\n",
    "            \n",
    "            #check if last in the array is 1 (this tells me if the anomaly ends or not), otherwise predict this is a SuddenR\n",
    "            if my_model[-1] == 1:\n",
    "                \n",
    "                mm3 = [all(i) for i in my_model.reshape(2, -1)]\n",
    "                #you can't differentiate SlowD from SuddenD other than that that SlowD starts very early in the array, so if second half of the array is all 1 predict Slow, otherwise predict this is a SuddenD\n",
    "                if any(mm3) == 1:\n",
    "                    \n",
    "                    return \"SlowD\"\n",
    "                return \"SuddenD\"\n",
    "            return \"SuddenR\"\n",
    "        return \"InstaD\"\n",
    "    return \"No_anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbc72f-35c7-4a17-b3ce-1ed2695691a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Anomaly_loc(pred_type,pred_pred):\n",
    "    W_list = []\n",
    "    for i in range(len(pred_type)):\n",
    "        \n",
    "        result = [j for j, x in enumerate(pred_model[i]) if x]\n",
    "        \n",
    "        if pred_type[i] == \"No_anomaly\":\n",
    "            W = \"There is no anomaly\"\n",
    "            \n",
    "        elif pred_type[i] == \"InstaD\":\n",
    "            result_str = [str(a) for a in result]\n",
    "            W = \"Anomaly type is Instant Degredation and the anomalies are located at times \"+', '.join(result_str)\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenR\":\n",
    "            W = \"Anomaly type is Sudden Recovery and the anomalies are located at times from \" + str(result[0]) + \" to \" + str (result[-1])\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenD\":\n",
    "            W = \"Anomaly type is Sudden Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        \n",
    "        elif pred_type[i] == \"SlowD\":\n",
    "            W = \"Anomaly type is Slow Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        W_list = np.append(W_list, W)\n",
    "\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28b1a5-dd11-4890-bf67-fc3be990462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = pd.read_csv(\"datasets/pred_array.csv\") \n",
    "true_model = pd.read_csv(\"datasets/true_array.csv\")\n",
    "del pred_model['Unnamed: 0']\n",
    "del true_model['Unnamed: 0']\n",
    "pred_model = np.array(pred_model).reshape(1699, -1)\n",
    "true_model = np.array(true_model).reshape(1699, -1)\n",
    "\n",
    "# pred_array2 = np.array([])\n",
    "# for i in pred_model:\n",
    "#     pred_array2 = np.append(pred_array2, anomaly_type(i))\n",
    "\n",
    "# true_array2 = np.array([])\n",
    "# for i in true_model:\n",
    "#     true_array2 = np.append(true_array2, anomaly_type(i))\n",
    "    \n",
    "# Tell_me = Anomaly_loc(pred_array2, pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e1253-1485-460a-85ae-ca5ae853510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=9\n",
    "print(pred_model[i])\n",
    "print(true_model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08c35b-bd28-4620-ae81-2d4200190549",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tell_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267f23a-445b-4699-bc03-8a1ed540bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "        dataset_rss = np.load(\"datasets/dataset_rss_50_300.npz\", allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(\"datasets/dataset_properties_50_300.npz\", allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(\"datasets/dataset_mask_50_300.npz\", allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            dataset_properties[i,1] = int(dataset_properties[i,1])\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e76611-53dc-4056-ad66-4b40887f38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y_len)\n",
    "ax = df.plot.hist(bins=100, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48896e3-57c7-4254-9da6-f1d0d4c26505",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"vis\"\n",
    "classif = \"graph\" \n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "#paths\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "output = generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766e6a7-4ee0-45b4-867f-defba9f9fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        k = 7005\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        \n",
    "        g.build(X[k])\n",
    "\n",
    "        adj_mat_vis = np.zeros([len(X[k]),len(X[k])], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_vis[x,y] = q\n",
    "            adj_mat_vis[y,x] = q\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_vis).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_vis[row, col]\n",
    "        plt.imshow(adj_mat_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaeaeac-633b-4135-8f42-01d118dc39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e34e40-ecdb-4aba-aaa1-824931a8c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=output[5012]\n",
    "\n",
    "edge_index = pd.DataFrame(df.edge_index.numpy())\n",
    "x = pd.DataFrame(df.x.numpy())[:20].T\n",
    "\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"RSSI\")\n",
    "\n",
    "col =[]\n",
    "\n",
    "for i in range(len(x.T)):\n",
    "    if x[i][0]<-80:\n",
    "        col.append('red')  \n",
    "    else:\n",
    "        col.append('blue') \n",
    "\n",
    "#plt.plot(x.T,linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(edge_index.T[:78-44])):\n",
    "    plt.plot(edge_index[i],[x[edge_index[i][0]][0],x[edge_index[i][1]][0]],c='grey',linewidth=0.5)\n",
    "for i in range(len(x.T)):\n",
    "    plt.plot(i,x[i].T, marker=\".\",markersize=15,  c = col[i])\n",
    "    \n",
    "plt.savefig('vis_plot.eps', format='eps')\n",
    "plt.savefig('vis_plot.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efe60b-7148-4286-a698-0ae8092223ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k=8002\n",
    "\n",
    "datax = output[k]\n",
    "datax.x = datax.x#[:20]\n",
    "datax.edge_index = datax.edge_index#[:,:78-44]\n",
    "datax.edge_attr = datax.edge_attr#[:78-44]\n",
    "mask = X_mask[k]#[:20]\n",
    "color_map = []\n",
    "for i in range(len(mask)):\n",
    "    if mask[i] == 0:\n",
    "        color_map.append('yellow')\n",
    "    else: \n",
    "        color_map.append('red')     \n",
    "g = to_networkx(data = datax, to_undirected = True)\n",
    "nx.draw(g, node_size=500, with_labels=True, node_color = color_map)\n",
    "\n",
    "plt.savefig('graph_plot.eps', format='eps')\n",
    "plt.savefig('graph_plot.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76975c-66b1-417b-bb8c-4c9fb360ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[4011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e951058-7dad-481c-b3cf-8f90c3649047",
   "metadata": {},
   "outputs": [],
   "source": [
    "datax.edge_index[:,:78-32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebce41-a400-486d-84cd-9af94351ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required module\n",
    "import timeit\n",
    " \n",
    "# code snippet to be executed only once\n",
    "SETUP_CODE = '''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ts2vg\n",
    "from pyts.image import MarkovTransitionField\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    " '''\n",
    "\n",
    "# code snippet whose execution time is to be measured\n",
    "TEST_CODE = '''\n",
    "graph_type = \"MTF\"\n",
    "classif = \"graph\" \n",
    "len_type = \"un/cut\" \n",
    "vis_type = \"natural\"\n",
    "vis_distance = 'distance'\n",
    "vis_edge_type = \"undirected\"\n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\"\n",
    "        \n",
    "df = pd.read_csv(path_main)  \n",
    "del df['Unnamed: 0']\n",
    "df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "length_rss = int((df.columns.stop-2)/2)\n",
    "global X_mask\n",
    "X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "if graph_type in (\"MTF\", \"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "    MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "    X_gaf_MTF = MTF.fit_transform(X)\n",
    "output = []\n",
    "\n",
    "def vis_matrix(X_current, vis_type_local):\n",
    "    if vis_type_local == \"natural\":\n",
    "        g = NaturalVG(weighted=vis_distance)\n",
    "    elif vis_type_local == \"horizontal\":\n",
    "        g = HorizontalVG(weighted=vis_distance)\n",
    "\n",
    "    g.build(X_current)\n",
    "\n",
    "    adj_mat_vis = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "    for i in range(len(g.edges)):\n",
    "        x, y, q =g.edges[i]\n",
    "        adj_mat_vis[x,y] = q\n",
    "        if vis_edge_type == \"undirected\":\n",
    "            adj_mat_vis[y,x] = q\n",
    "\n",
    "    return adj_mat_vis\n",
    "\n",
    "# functions for creating edge index and edge weight for a given matrix\n",
    "def adjToEdgidx(adj_mat):\n",
    "    #function for visibility and MTF matrixes\n",
    "    edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "    row, col = edge_index\n",
    "    edge_weight = adj_mat[row, col]\n",
    "    return edge_index,edge_weight\n",
    "\n",
    "def define_mask(i):\n",
    "    if classif == \"graph\": # for graph classification\n",
    "        return torch.tensor(Y[i], dtype=torch.long)      \n",
    "    elif classif == \"node\":# for node classification \n",
    "        return torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "\n",
    "if graph_type == \"MTF\":  \n",
    "    for i, j in enumerate(X_gaf_MTF):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        y_mask = define_mask(i)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally label\n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "elif graph_type == \"vis\":\n",
    "    for i in range(len(X)):\n",
    "        edge_index, edge_weight = adjToEdgidx(vis_matrix(X[i],vis_type))\n",
    "        y_mask = define_mask(i)\n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=torch.tensor(edge_index, dtype=torch.int64), edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),y=y_mask))    \n",
    "'''\n",
    " \n",
    "# timeit statement\n",
    "print (timeit.repeat(setup = SETUP_CODE,\n",
    "                     stmt = TEST_CODE,\n",
    "                     repeat = 5,\n",
    "                     number = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1da543-393c-4e5b-a370-08533936a5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd91489-37bf-4f64-abd4-151dbf654958",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2123\n",
    "d = 4\n",
    "\n",
    "df0=output[k*0+d]\n",
    "df1=output[k*1+d]\n",
    "df2=output[k*2+d]\n",
    "df3=output[k*3+d]\n",
    "df4=output[k*4+d]\n",
    "edge_index = pd.DataFrame(df.edge_index.numpy())\n",
    "x0 = pd.DataFrame(df0.x.numpy()).T\n",
    "x1 = pd.DataFrame(df1.x.numpy()).T\n",
    "x2 = pd.DataFrame(df2.x.numpy()).T\n",
    "x3 = pd.DataFrame(df3.x.numpy()).T\n",
    "x4 = pd.DataFrame(df4.x.numpy()).T\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"RSSI\")\n",
    "\n",
    "col =[]\n",
    "\n",
    "# for i in range(len(x1.T)):\n",
    "#     if x1[i][0]<-80:\n",
    "#         col.append('red')  \n",
    "#     else:\n",
    "#         col.append('blue') \n",
    "\n",
    "#plt.plot(x.T,linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(edge_index.T)):\n",
    "#     plt.plot(edge_index[i],[x[edge_index[i][0]][0],x[edge_index[i][1]][0]],c='grey',linewidth=0.5)\n",
    "#for i in range(len(x.T)):\n",
    "plt.plot(x0.T, marker=\".\",markersize=1,  c = \"red\")\n",
    "plt.plot(x1.T, marker=\".\",markersize=1,  c = \"blue\")\n",
    "plt.plot(x2.T, marker=\".\",markersize=1,  c = \"green\")\n",
    "plt.plot(x3.T, marker=\".\",markersize=1,  c = \"black\")\n",
    "plt.plot(x4.T, marker=\".\",markersize=1,  c = \"yellow\")\n",
    "    \n",
    "plt.savefig('vis_plot.eps', format='eps')\n",
    "plt.savefig('vis_plot.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f93b6-5172-438e-8bbe-9d200ca62c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = list(range(300)) \n",
    "x = df3.x.numpy().T[0]\n",
    "plt.plot(y, x, '-o')\n",
    "plt.fill_between(y, x, color='blue', alpha=0.5, where=(y > y.mean()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d59336-f5e5-4163-af01-e20ac916179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0b0ac-f489-472a-b74e-d78d05011052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
