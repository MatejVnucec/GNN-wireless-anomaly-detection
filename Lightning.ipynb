{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0d00-7f18-4047-83e9-263316278bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install ts2vg\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pyts.image import MarkovTransitionField\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import ts2vg\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1507f595-2818-4106-bfa8-6c2b85d1668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for graph creation\n",
    "def create_MTF_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]#adj_mat[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123e5ab4-575b-4265-ac9d-99e6318a6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #function for graph creation\n",
    "def create_visual_graph():\n",
    "    # warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        \n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        # g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_visual = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_visual).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_visual[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        edge_index, edge_weight = adjToEdgidx(X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                           edge_index=torch.tensor(edge_index, dtype=torch.int64), \n",
    "                           edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),\n",
    "                           y=y_mask))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eca4cf2-1904-4169-afa1-4bbebbd9554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_join_graph():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "        \n",
    "    def adjToEdgidx(adj_mat_MTF,X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual = np.zeros([len(adj_mat_MTF),len(adj_mat_MTF)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_MTF).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual[row, col]\n",
    "        \n",
    "        # edge_weight = np.reshape(edge_weight,(len(edge_weight),2))\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j,X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels       \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                    edge_index=edge_index, \n",
    "                    edge_attr=torch.tensor(edge_weight, dtype=torch.double), \n",
    "                    y=y_mask))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c4f32d8-da08-4baf-a8ab-56aba5f0ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Focal loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab96923-16af-4733-a746-8b9a47f7a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
    "\n",
    "    def __init__(self, gamma=1, alpha=None, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input_, target):\n",
    "        cross_entropy = super().forward(input_, target)\n",
    "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
    "        # This won't contribute final loss as the cross_entropy contribution\n",
    "        # for these would be zero.\n",
    "        target = target * (target != self.ignore_index).long()\n",
    "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
    "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
    "        \n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss) \n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss) \n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the last output so we dont have to generate another if all parameters are the same as beffore\n",
    "global temp_repeat\n",
    "temp_repeat=['']*6\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask:\n",
    "    \n",
    "        global output\n",
    "        if graph_type == \"MTF\":\n",
    "            output = create_MTF_graph()\n",
    "        if graph_type == \"visual\":\n",
    "            output = create_visual_graph()\n",
    "    \n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4e3cda64-29f2-4739-a877-ddf5526a9292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | conv1 | GINEConv | 2.2 K \n",
      "1 | conv2 | GINEConv | 2.2 K \n",
      "2 | conv3 | GINEConv | 2.2 K \n",
      "3 | conv4 | GINEConv | 2.2 K \n",
      "4 | conv5 | GINEConv | 2.2 K \n",
      "5 | lin1  | Linear   | 25.8 K\n",
      "6 | lin2  | Linear   | 805   \n",
      "-----------------------------------\n",
      "37.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "37.8 K    Total params\n",
      "0.151     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaee367df4fd4b63bd09a133b9e51566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lightning Module\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import random_split\n",
    "# from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, dim_h):\n",
    "        super(LitAutoEncoder, self).__init__()\n",
    "        edge_dim = 1\n",
    "        \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \n",
    "#         total_loss = 0\n",
    "#         for data in train_loader:\n",
    "#             data = data.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = loss_op(model(data.x.float(), data.edge_index), data.y)\n",
    "#             total_loss += loss.item() * len(train_loader)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         return total_loss / len(train_loader.dataset)\n",
    "#         for data in val_batch:\n",
    "        \n",
    "#             x, edge_index = data.x, data.edge_index\n",
    "#             batch_index = batch_idx\n",
    "\n",
    "#             x_out = self.forward(x, edge_index, batch_index)\n",
    "\n",
    "#             loss_function = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "#             loss = loss_function(x_out, data.y)\n",
    "\n",
    "#             # metrics here\n",
    "#             pred = x_out.argmax(-1)\n",
    "#             label = batch.y\n",
    "#             accuracy = (pred == label).sum() / pred.shape[0]\n",
    "            \n",
    "#             total_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "#         loss = total_loss / len(loader.dataset)\n",
    "#         self.log(\"loss/train\", loss)\n",
    "#         self.log(\"accuracy/train\", accuracy)\n",
    "\n",
    "#         return loss\n",
    "# -------------------------------------------       \n",
    "              \n",
    "        self.on_batch_start()\n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device) #weight=class_weights\n",
    "        loss = loss_function(out, train_batch.y)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        # class_weights.type_as(\"gpu\")\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        pred = out.argmax(-1)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        # class_weights.type_as(\"gpu\")\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_loss\", test_label)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        \n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"Training is starting\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training is ending\")\n",
    "\n",
    "\n",
    "trainer = Trainer(callbacks=[MyPrintingCallback()])\n",
    "\n",
    "# my data        \n",
    "output = generate_output()\n",
    "torch.manual_seed(6406)\n",
    "\n",
    "train_size = int(0.6 * len(output))\n",
    "Temp_size = len(output) - train_size\n",
    "val_size = int(0.5*Temp_size)\n",
    "test_size = Temp_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "# model\n",
    "model = LitAutoEncoder(32).double()#.to(device)\n",
    "#training\n",
    "# trainer = pl.Trainer(accelerator='gpu', devices=1, precision=16, limit_train_batches=0.5)\n",
    "val_check_interval=len(train_loader)\n",
    "trainer = pl.Trainer(max_epochs = 10, accelerator='gpu', devices=1, val_check_interval=val_check_interval) #gpus=[0])\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer = Trainer(logger=logger)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1dea6b3-9f21-4664-b319-5d7fb5000cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f0700a6df44a309d1c02f21f591a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('test_loss', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7626001238822937\n",
      "        test_loss           1.9759774208068848\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.9759774208068848, 'test_acc': 0.7626001238822937}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
