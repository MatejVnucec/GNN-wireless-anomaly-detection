{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71e8e40-5bd1-4cdc-ac0e-0ac3088277ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e59a55-999c-48f2-822b-92535186360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING conda.lock:touch(51): Failed to create lock, do not run conda in parallel processes [errno 13]\n",
      "  Package              Version  Build                        Channel                    Size\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  Install:\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\u001b[32m  + cudatoolkit    \u001b[00m     11.3.1  h9edb442_11                  conda-forge/linux-64     548 MB\n",
      "\u001b[32m  + ffmpeg         \u001b[00m        4.3  hf484d3e_0                   pytorch/linux-64          10 MB\n",
      "\u001b[32m  + fmt            \u001b[00m      9.1.0  h924138e_0                   conda-forge/linux-64     185 KB\n",
      "\u001b[32m  + gnutls         \u001b[00m     3.6.13  h85f3911_1                   conda-forge/linux-64       2 MB\n",
      "\u001b[32m  + keyutils       \u001b[00m      1.6.1  h166bdaf_0                   conda-forge/linux-64     115 KB\n",
      "\u001b[32m  + lame           \u001b[00m      3.100  h166bdaf_1003                conda-forge/linux-64     496 KB\n",
      "\u001b[32m  + libmamba       \u001b[00m      1.1.0  h70b1f8a_2                   conda-forge/linux-64       1 MB\n",
      "\u001b[32m  + libmambapy     \u001b[00m      1.1.0  py39hf968b3e_2               conda-forge/linux-64     307 KB\n",
      "\u001b[32m  + mkl            \u001b[00m   2022.1.0  h84fe81f_915                 conda-forge/linux-64     200 MB\n",
      "\u001b[32m  + mkl-devel      \u001b[00m   2022.1.0  ha770c72_916                 conda-forge/linux-64      25 KB\n",
      "\u001b[32m  + mkl-include    \u001b[00m   2022.1.0  h84fe81f_915                 conda-forge/linux-64     745 KB\n",
      "\u001b[32m  + nettle         \u001b[00m        3.6  he412f7d_0                   conda-forge/linux-64       6 MB\n",
      "\u001b[32m  + openh264       \u001b[00m      2.1.1  h780b84a_0                   conda-forge/linux-64       2 MB\n",
      "\u001b[32m  + pybind11-abi   \u001b[00m          4  hd8ed1ab_3                   conda-forge/noarch        10 KB\n",
      "\u001b[32m  + pytorch        \u001b[00m     1.12.1  py3.9_cuda11.3_cudnn8.3.2_0  pytorch/linux-64           1 GB\n",
      "\u001b[32m  + pytorch-mutex  \u001b[00m        1.0  cuda                         pytorch/noarch             3 KB\n",
      "\u001b[32m  + tbb            \u001b[00m   2021.7.0  h924138e_0                   conda-forge/linux-64       2 MB\n",
      "\u001b[32m  + torchaudio     \u001b[00m     0.12.1  py39_cu113                   pytorch/linux-64           6 MB\n",
      "\u001b[32m  + torchvision    \u001b[00m     0.13.1  py39_cu113                   pytorch/linux-64           7 MB\n",
      "\u001b[32m  + yaml-cpp       \u001b[00m      0.7.0  h27087fc_2                   conda-forge/linux-64     215 KB\n",
      "\n",
      "  Change:\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\u001b[31m  - blas-devel     \u001b[00m      3.9.0  12_linux64_openblas          installed                      \n",
      "\u001b[32m  + blas-devel     \u001b[00m      3.9.0  16_linux64_mkl               conda-forge/linux-64      12 KB\n",
      "\u001b[31m  - hdf5           \u001b[00m     1.12.1  nompi_h2750804_101           installed                      \n",
      "\u001b[32m  + hdf5           \u001b[00m     1.12.1  nompi_h4df4325_104           conda-forge/linux-64       4 MB\n",
      "\u001b[31m  - libarchive     \u001b[00m      3.5.2  hccf745f_1                   installed                      \n",
      "\u001b[32m  + libarchive     \u001b[00m      3.5.2  hed592e5_1                   conda-forge/linux-64       2 MB\n",
      "\u001b[31m  - libblas        \u001b[00m      3.9.0  12_linux64_openblas          installed                      \n",
      "\u001b[32m  + libblas        \u001b[00m      3.9.0  16_linux64_mkl               conda-forge/linux-64      13 KB\n",
      "\u001b[31m  - libcblas       \u001b[00m      3.9.0  12_linux64_openblas          installed                      \n",
      "\u001b[32m  + libcblas       \u001b[00m      3.9.0  16_linux64_mkl               conda-forge/linux-64      12 KB\n",
      "\u001b[31m  - libgit2        \u001b[00m      1.3.0  hee63804_1                   installed                      \n",
      "\u001b[32m  + libgit2        \u001b[00m      1.3.0  haabb1ae_1                   conda-forge/linux-64     721 KB\n",
      "\u001b[31m  - liblapack      \u001b[00m      3.9.0  12_linux64_openblas          installed                      \n",
      "\u001b[32m  + liblapack      \u001b[00m      3.9.0  16_linux64_mkl               conda-forge/linux-64      12 KB\n",
      "\u001b[31m  - liblapacke     \u001b[00m      3.9.0  12_linux64_openblas          installed                      \n",
      "\u001b[32m  + liblapacke     \u001b[00m      3.9.0  16_linux64_mkl               conda-forge/linux-64      12 KB\n",
      "\u001b[31m  - libssh2        \u001b[00m     1.10.0  ha56f1ee_2                   installed                      \n",
      "\u001b[32m  + libssh2        \u001b[00m     1.10.0  hf14f497_3                   conda-forge/linux-64     234 KB\n",
      "\u001b[31m  - libxml2        \u001b[00m     2.9.12  h72842e0_0                   installed                      \n",
      "\u001b[32m  + libxml2        \u001b[00m     2.9.12  h72842e0_0                   conda-forge/linux-64     772 KB\n",
      "\u001b[31m  - python         \u001b[00m      3.9.7  hb7a2778_3_cpython           installed                      \n",
      "\u001b[32m  + python         \u001b[00m      3.9.7  hf930737_3_cpython           conda-forge/linux-64      28 MB\n",
      "\n",
      "  Upgrade:\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\u001b[31m  - blas           \u001b[00m      2.112  openblas                     installed                      \n",
      "\u001b[32m  + blas           \u001b[00m      2.116  mkl                          conda-forge/linux-64      13 KB\n",
      "\u001b[31m  - ca-certificates\u001b[00m  2021.10.8  ha878542_0                   installed                      \n",
      "\u001b[32m  + ca-certificates\u001b[00m  2022.12.7  ha878542_0                   conda-forge/linux-64     143 KB\n",
      "\u001b[31m  - certifi        \u001b[00m  2021.10.8  py39hf3d152e_1               installed                      \n",
      "\u001b[32m  + certifi        \u001b[00m  2022.12.7  pyhd8ed1ab_0                 conda-forge/noarch       147 KB\n",
      "\u001b[31m  - cryptography   \u001b[00m     35.0.0  py39h95dcef6_2               installed                      \n",
      "\u001b[32m  + cryptography   \u001b[00m     38.0.4  py39h3ccb8fc_0               conda-forge/linux-64       1 MB\n",
      "\u001b[31m  - curl           \u001b[00m     7.80.0  h2574ce0_0                   installed                      \n",
      "\u001b[32m  + curl           \u001b[00m     7.86.0  h2283fc2_1                   conda-forge/linux-64      91 KB\n",
      "\u001b[31m  - krb5           \u001b[00m     1.19.2  hcc1bbae_3                   installed                      \n",
      "\u001b[32m  + krb5           \u001b[00m     1.19.3  h08a2579_0                   conda-forge/linux-64       1 MB\n",
      "\u001b[31m  - libcurl        \u001b[00m     7.80.0  h2574ce0_0                   installed                      \n",
      "\u001b[32m  + libcurl        \u001b[00m     7.86.0  h2283fc2_1                   conda-forge/linux-64     351 KB\n",
      "\u001b[31m  - libgcc-ng      \u001b[00m     11.2.0  h1d223b6_11                  installed                      \n",
      "\u001b[32m  + libgcc-ng      \u001b[00m     12.2.0  h65d4601_19                  conda-forge/linux-64     931 KB\n",
      "\u001b[31m  - libglib        \u001b[00m     2.70.1  h174f98d_0                   installed                      \n",
      "\u001b[32m  + libglib        \u001b[00m     2.74.0  h7a41b64_0                   conda-forge/linux-64       3 MB\n",
      "\u001b[31m  - libgomp        \u001b[00m     11.2.0  h1d223b6_11                  installed                      \n",
      "\u001b[32m  + libgomp        \u001b[00m     12.2.0  h65d4601_19                  conda-forge/linux-64     455 KB\n",
      "\u001b[31m  - libiconv       \u001b[00m       1.16  h516909a_0                   installed                      \n",
      "\u001b[32m  + libiconv       \u001b[00m       1.17  h166bdaf_0                   conda-forge/linux-64       1 MB\n",
      "\u001b[31m  - libnghttp2     \u001b[00m     1.43.0  h812cca2_1                   installed                      \n",
      "\u001b[32m  + libnghttp2     \u001b[00m     1.47.0  hff17c54_1                   conda-forge/linux-64     824 KB\n",
      "\u001b[31m  - libsolv        \u001b[00m     0.7.19  h780b84a_5                   installed                      \n",
      "\u001b[32m  + libsolv        \u001b[00m     0.7.22  h6239696_0                   conda-forge/linux-64     443 KB\n",
      "\u001b[31m  - libstdcxx-ng   \u001b[00m     11.2.0  he4da1e4_11                  installed                      \n",
      "\u001b[32m  + libstdcxx-ng   \u001b[00m     12.2.0  h46fd767_19                  conda-forge/linux-64       4 MB\n",
      "\u001b[31m  - libzlib        \u001b[00m     1.2.11  h36c2ea0_1013                installed                      \n",
      "\u001b[32m  + libzlib        \u001b[00m     1.2.13  h166bdaf_4                   conda-forge/linux-64      64 KB\n",
      "\u001b[31m  - llvm-openmp    \u001b[00m     12.0.1  h4bd325d_1                   installed                      \n",
      "\u001b[32m  + llvm-openmp    \u001b[00m     15.0.6  he0ac6c6_0                   conda-forge/linux-64       3 MB\n",
      "\u001b[31m  - mamba          \u001b[00m     0.17.0  py39h951de11_0               installed                      \n",
      "\u001b[32m  + mamba          \u001b[00m      1.1.0  py39hc5d2bb1_2               conda-forge/linux-64      47 KB\n",
      "\u001b[31m  - openssl        \u001b[00m     1.1.1l  h7f98852_0                   installed                      \n",
      "\u001b[32m  + openssl        \u001b[00m      3.0.7  h0b41bf4_1                   conda-forge/linux-64       2 MB\n",
      "\u001b[31m  - pycurl         \u001b[00m     7.44.1  py39h72e3413_1               installed                      \n",
      "\u001b[32m  + pycurl         \u001b[00m     7.45.1  py39h9297c8b_3               conda-forge/linux-64      74 KB\n",
      "\u001b[31m  - pyjwt          \u001b[00m      2.3.0  pyhd8ed1ab_0                 installed                      \n",
      "\u001b[32m  + pyjwt          \u001b[00m      2.6.0  pyhd8ed1ab_0                 conda-forge/noarch        21 KB\n",
      "\u001b[31m  - r-git2r        \u001b[00m     0.28.0  r41hf628c3e_1                installed                      \n",
      "\u001b[32m  + r-git2r        \u001b[00m     0.30.1  r41hf72769b_1                conda-forge/linux-64     949 KB\n",
      "\u001b[31m  - r-haven        \u001b[00m      2.4.3  r41h2713e49_0                installed                      \n",
      "\u001b[32m  + r-haven        \u001b[00m      2.5.1  r41h7525677_0                conda-forge/linux-64     410 KB\n",
      "\u001b[31m  - r-openssl      \u001b[00m      1.4.5  r41he36bf35_1                installed                      \n",
      "\u001b[32m  + r-openssl      \u001b[00m      2.0.5  r41habfbb5e_0                conda-forge/linux-64     620 KB\n",
      "\u001b[31m  - r-readxl       \u001b[00m      1.3.1  r41h2713e49_4                installed                      \n",
      "\u001b[32m  + r-readxl       \u001b[00m      1.4.1  r41h3ebcfa7_1                conda-forge/linux-64     875 KB\n",
      "\u001b[31m  - unixodbc       \u001b[00m      2.3.9  hb166930_0                   installed                      \n",
      "\u001b[32m  + unixodbc       \u001b[00m     2.3.10  h583eb01_0                   conda-forge/linux-64     296 KB\n",
      "\u001b[31m  - zlib           \u001b[00m     1.2.11  h36c2ea0_1013                installed                      \n",
      "\u001b[32m  + zlib           \u001b[00m     1.2.13  h166bdaf_4                   conda-forge/linux-64      92 KB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 20 packages\n",
      "  Change: 11 packages\n",
      "  Upgrade: 26 packages\n",
      "\n",
      "  Total download: 2 GB\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-scatter\n",
      "  Using cached https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp39-cp39-linux_x86_64.whl (8.9 MB)\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.0+pt112cu113\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-sparse\n",
      "  Using cached https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15%2Bpt112cu113-cp39-cp39-linux_x86_64.whl (3.5 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from torch-sparse) (1.7.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.9/site-packages (from scipy->torch-sparse) (1.20.3)\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.15+pt112cu113\n",
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (1.7.2)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (2.4.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (2.26.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (5.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (4.62.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torch-geometric) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->torch-geometric) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llvmpy\n",
      "  Using cached llvmpy-0.12.7.tar.gz (657 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_3749ddb5e7a846f5a33e15e28736173f/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_3749ddb5e7a846f5a33e15e28736173f/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-qdf_tdl1\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_3749ddb5e7a846f5a33e15e28736173f/\n",
      "  Complete output (2 lines):\n",
      "  Error: could not invoke ['llvm-config', '--version']\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/97/0f/98b78dc8a5ec032e05179fc406622e550d5c571f036beca8f06a4752f648/llvmpy-0.12.7.tar.gz#sha256=2f0f37aa7966d267a9ababe59f7f3c861a5b7b928205c9137275f7032f4132c7 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.12.6.tar.gz (654 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_1c632d0d65fe454baced9e29c9b1c861/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_1c632d0d65fe454baced9e29c9b1c861/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-y0iba3i6\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_1c632d0d65fe454baced9e29c9b1c861/\n",
      "  Complete output (2 lines):\n",
      "  Error: could not invoke ['llvm-config', '--version']\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/61/97/ad6be5dd5c7753fc1839433a3b6944ca6305b9f50a19f6462c0703c30e38/llvmpy-0.12.6.tar.gz#sha256=a468a4b2c3c969fa33a5151c6f99acc31dcbce6cd32c13a7b029096101f47de8 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.12.5.tar.gz (659 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_c31cf0f7adb74c7aaa0a21fc12c01efd/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_c31cf0f7adb74c7aaa0a21fc12c01efd/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-95gdg2f9\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_c31cf0f7adb74c7aaa0a21fc12c01efd/\n",
      "  Complete output (2 lines):\n",
      "  Error: could not invoke ['llvm-config', '--version']\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/79/9e/4096e02a789d1beb719f9a0be476907bc9508a0e2b463e32694dbbced422/llvmpy-0.12.5.tar.gz#sha256=b544fc1cad3f7b7e6e40482b00405dffb7c0dbd60adc03fb395dfb8d5b3785a2 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.12.4.tar.gz (651 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_de90b59d7d8e413fb6c403fa2506b707/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_de90b59d7d8e413fb6c403fa2506b707/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-huq73_na\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_de90b59d7d8e413fb6c403fa2506b707/\n",
      "  Complete output (5 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-_z_dctbk/llvmpy_de90b59d7d8e413fb6c403fa2506b707/setup.py\", line 7, in <module>\n",
      "      import versioneer\n",
      "  ModuleNotFoundError: No module named 'versioneer'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ad/06/5746eb3fe8b1d487dd7ade959fc43d61d9a9eadfabc80ec2ecae80252727/llvmpy-0.12.4.tar.gz#sha256=399ab16f613fb9f742e8338701c2e1c605817b2aab36cc2b202e0b681ef7561c (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.11.2.tar.gz (642 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_8ba0a3d6189b4afe9d61f71f542e7041/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_8ba0a3d6189b4afe9d61f71f542e7041/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-qsg5ivuz\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_8ba0a3d6189b4afe9d61f71f542e7041/\n",
      "  Complete output (2 lines):\n",
      "  Error: could not invoke ['llvm-config', '--version']\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/88/99/1f75d3b46f4f732b70c7ece77d0143036d29683e39eb09a383297055a7a0/llvmpy-0.11.2.tar.gz#sha256=c7911b743d4d3ac746eca6d0c0809963fc326bdc079a27fb2e3053e0474fa299 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.11.0.tar.gz (637 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_c8742220249a4335aeacda9ca9bc2985/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_c8742220249a4335aeacda9ca9bc2985/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-zlv4qlbz\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_c8742220249a4335aeacda9ca9bc2985/\n",
      "  Complete output (2 lines):\n",
      "  Error: could not invoke ['llvm-config', '--version']\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/96/68/510e540e6d63753c56191654cd26a002cd2b215cd7e5e5e81a7b4cb1e3cd/llvmpy-0.11.0.tar.gz#sha256=59165fbf2e3aae5642c3b7abe56296bf1b90607ab2fa2192d16f7478f8cc7760 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.10.2.tar.gz (555 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_6f70795adbe947f59638f7c7584b26ff/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_6f70795adbe947f59638f7c7584b26ff/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-iga9o6b1\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_6f70795adbe947f59638f7c7584b26ff/\n",
      "  Complete output (3 lines):\n",
      "  /bin/sh: 1: llvm-config: not found\n",
      "  Cannot invoke llvm-config.\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/35/bc/46b99038319e7ea3a0240e5f06ba609f8d195899a1fe442840fff9117027/llvmpy-0.10.2.tar.gz#sha256=bf48203483e8f521ed8181770bf41d031faf3ff2c168f9716d35e178f3991c7a (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.10.1.tar.gz (546 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_56d03ffe6685473f9dd1157e44d771f4/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_56d03ffe6685473f9dd1157e44d771f4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-nezr8i1s\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_56d03ffe6685473f9dd1157e44d771f4/\n",
      "  Complete output (3 lines):\n",
      "  /bin/sh: 1: llvm-config: not found\n",
      "  Cannot invoke llvm-config.\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6e/70/bcc156a448def3e7f40f22dff7df175591a4b56a192258a71c02424e70fb/llvmpy-0.10.1.tar.gz#sha256=2225fca4080dd43b88143db75f52a5a02af26269138ff467844740187cce0c4e (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.10.0.tar.gz (540 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_9ed3ff9f9235444b96eaeadd0d03223c/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_9ed3ff9f9235444b96eaeadd0d03223c/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-dqc9khj7\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_9ed3ff9f9235444b96eaeadd0d03223c/\n",
      "  Complete output (3 lines):\n",
      "  /bin/sh: 1: llvm-config: not found\n",
      "  Cannot invoke llvm-config.\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/94/3c/8563b6f88d225f530ad1cd21820c448df15e621fdf66e5b585ca09365009/llvmpy-0.10.0.tar.gz#sha256=13699df4b4ce8b0ad34762cc568e651471af22cd1e002d8e47d3a127855c7f25 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.9.tar.gz (414 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_2d1ac1204bdf49a0a697d338f52b5a93/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_2d1ac1204bdf49a0a697d338f52b5a93/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-excqy712\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_2d1ac1204bdf49a0a697d338f52b5a93/\n",
      "  Complete output (3 lines):\n",
      "  /bin/sh: 1: llvm-config: not found\n",
      "  Cannot invoke llvm-config.\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/2d/14/5c151ed4ca7d9c7e823b280a02f20775516a177973fc855b03bb6ffb31de/llvmpy-0.9.tar.gz#sha256=f77741344879b7801922e5b85c1b83bc19b08fb1276466267d37253f4c9a4673 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached llvmpy-0.8.2.tar.gz (486 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.9 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_ff4d3a1eca04496090a21945331bea39/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-_z_dctbk/llvmpy_ff4d3a1eca04496090a21945331bea39/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-cpd_0hfh\n",
      "       cwd: /tmp/pip-install-_z_dctbk/llvmpy_ff4d3a1eca04496090a21945331bea39/\n",
      "  Complete output (3 lines):\n",
      "  /bin/sh: 1: llvm-config: not found\n",
      "  Cannot invoke llvm-config.\n",
      "  Try setting LLVM_CONFIG_PATH=/path/to/llvm-config\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/40/f6/9b1c48a88089980ac8e14f50cb53a94813dbb1deb5858f8d64b2a15e6a2c/llvmpy-0.8.2.tar.gz#sha256=0ed4a86e760f94a9ccfc3dab5565161c61fa50e8c457842030e1787b26b7d758 (from https://pypi.org/simple/llvmpy/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement llvmpy (from versions: 0.12.7-9-g60b512d, 0.8.2, 0.9, 0.10.0, 0.10.1, 0.10.2, 0.11.0, 0.11.2, 0.12.4, 0.12.5, 0.12.6, 0.12.7)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for llvmpy\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cython in /opt/conda/lib/python3.9/site-packages (0.29.24)\n",
      "Requirement already satisfied: numba in /home/jovyan/.local/lib/python3.9/site-packages (0.56.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/jovyan/.local/lib/python3.9/site-packages (from numba) (0.39.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in /opt/conda/lib/python3.9/site-packages (from numba) (1.20.3)\n",
      "Requirement already satisfied: setuptools<60 in /opt/conda/lib/python3.9/site-packages (from numba) (59.1.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (2.6.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting ts2vg\n",
      "  Using cached ts2vg-1.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from ts2vg) (1.20.3)\n",
      "Installing collected packages: ts2vg\n",
      "Successfully installed ts2vg-1.0.0\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.8.4-py3-none-any.whl (799 kB)\n",
      "     |████████████████████████████████| 799 kB 7.2 MB/s            \n",
      "\u001b[?25hCollecting tensorboardX>=2.2\n",
      "  Using cached tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (1.12.1)\n",
      "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
      "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (6.0)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Using cached torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (21.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.9/site-packages (from pytorch_lightning) (2021.11.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.26.0)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.9/site-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch_lightning) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.7)\n",
      "Installing collected packages: multidict, frozenlist, yarl, typing-extensions, async-timeout, aiosignal, aiohttp, torchmetrics, tensorboardX, lightning-utilities, pytorch-lightning\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.4.2 multidict-6.0.3 pytorch-lightning-1.8.4 tensorboardX-2.5.1 torchmetrics-0.11.0 typing-extensions-4.4.0 yarl-1.8.2\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow) (21.2)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.19.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.28.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (59.1.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.7)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.1)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, MarkupSafe, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.0.1\n",
      "    Uninstalling MarkupSafe-2.0.1:\n",
      "      Successfully uninstalled MarkupSafe-2.0.1\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 werkzeug-2.2.2 wrapt-1.14.1\n",
      "Collecting dvclive\n",
      "  Using cached dvclive-1.1.1-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.11 in /opt/conda/lib/python3.9/site-packages (from dvclive) (0.17.17)\n",
      "Collecting dvc-render[table]>=0.0.12\n",
      "  Using cached dvc_render-0.0.15-py3-none-any.whl (18 kB)\n",
      "Collecting tabulate>=0.8.7\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from ruamel.yaml>=0.17.11->dvclive) (0.2.6)\n",
      "Installing collected packages: tabulate, dvc-render, dvclive\n",
      "Successfully installed dvc-render-0.0.15 dvclive-1.1.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install ts2vg\n",
    "!pip install pytorch_lightning\n",
    "!pip install tensorflow\n",
    "!pip install dvclive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64d07e-430e-43fd-8fd7-11a12531e4b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 09:01:00.767476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 09:01:01.298844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 09:01:01.298924: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-08 09:01:01.298932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import ts2vg\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pyts.image import MarkovTransitionField\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateFinder\n",
    "from pytorch_lightning.callbacks import BatchSizeFinder\n",
    "from pytorch_lightning.callbacks import ProgressBarBase\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "\n",
    "\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429b1b52-626e-4eb9-886a-e6f050c23778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all in one\n",
    "def create_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy() # x values for every sample\n",
    "        #X = np.round_(X,1)\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8) # types of anomalies\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy() # binary location of anomalies\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"visual_on_MTF\", \"MTF_on_visual\"):\n",
    "            MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "            X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            dataset_properties[i,1] = int(dataset_properties[i,1])\n",
    "        \n",
    "        X = dataset_rss # x values for every sample\n",
    "        X_mask = dataset_mask # binary location of anomalies\n",
    "        Y = dataset_properties[:,2] # types of anomalies\n",
    "        Y_len = dataset_properties[:,0] # length of every sample\n",
    "        if graph_type in (\"MTF\", \"visual_on_MTF\", \"MTF_on_visual\"):\n",
    "            X_gaf = []\n",
    "            for i in range(len(Y_len)):\n",
    "                MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "                X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "                X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx_MTF(adj_mat_MTF,X_current):\n",
    "        edge_index = torch.from_numpy(adj_mat_MTF).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_MTF[row, col]\n",
    "        return edge_index,edge_weight\n",
    "    \n",
    "    def adjToEdgidx_visual(X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_visual = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q\n",
    "            adj_mat_visual[y,x] = q\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_visual).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_visual[row, col]\n",
    "        \n",
    "        return edge_index,edge_weight\n",
    "    \n",
    "    def adjToEdgidx_join(adj_mat_MTF,X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual = np.zeros([len(adj_mat_MTF),len(adj_mat_MTF)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q\n",
    "            adj_mat_visual[y,x] = q\n",
    "            \n",
    "        if graph_type == \"visual_on_MTF\":\n",
    "            temp_main_adj_mat = adj_mat_MTF\n",
    "        elif graph_type == \"MTF_on_visual\":\n",
    "            temp_main_adj_mat = adj_mat_visual\n",
    "            \n",
    "        edge_index = torch.from_numpy(temp_main_adj_mat).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def adjToEdgidx_double_VG(X_current):\n",
    "        g1 = NaturalVG(weighted='distance')\n",
    "        g2 = HorizontalVG(weighted='distance')\n",
    "        g1.build(X_current)\n",
    "        g2.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual1 = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g1.edges)):\n",
    "            x, y, q =g1.edges[i]\n",
    "            adj_mat_visual1[x,y] = q\n",
    "            adj_mat_visual1[y,x] = q\n",
    "            \n",
    "        adj_mat_visual2 = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g2.edges)):\n",
    "            x, y, q =g2.edges[i]\n",
    "            adj_mat_visual2[x,y] = q\n",
    "            adj_mat_visual2[y,x] = q\n",
    "            \n",
    "\n",
    "        edge_index = torch.from_numpy(adj_mat_visual1).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_visual1[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual2[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def define_mask(i):\n",
    "        if classif == \"graph\": # for graph classification\n",
    "            return torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":# for node classification \n",
    "            return torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "    if graph_type == \"MTF\":  \n",
    "        for i, j in enumerate(X_gaf):\n",
    "            edge_index, edge_weight = adjToEdgidx_MTF(j)\n",
    "            y_mask = define_mask(i)\n",
    "            #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally label\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    elif graph_type == \"visual\":\n",
    "        for i in range(len(X)):\n",
    "            edge_index, edge_weight = adjToEdgidx_visual(X[i])\n",
    "            y_mask = define_mask(i)\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=torch.tensor(edge_index, dtype=torch.int64), edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),y=y_mask))    \n",
    "    elif graph_type in (\"visual_on_MTF\", \"MTF_on_visual\"):\n",
    "        for i, j in enumerate(X_gaf):\n",
    "            edge_index, edge_weight = adjToEdgidx_join(j, X[i])\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    elif graph_type == \"double_VG\":\n",
    "        for i in range(len(X)):\n",
    "            edge_index, edge_weight = adjToEdgidx_double_VG(X[i])\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    return output    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the last output so we dont have to generate another if all parameters are the same as before\n",
    "global temp_repeat\n",
    "temp_repeat=['']*6\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask:\n",
    "    \n",
    "        global output\n",
    "        output = create_graph()\n",
    "            \n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    \n",
    "    print(temp_repeat[1] + \" classification on \" + temp_repeat[2] + \" time series using \" + temp_repeat[0] + \" graphs\" )\n",
    "    \n",
    "    return output\n",
    "\n",
    "def check_for_missclick():\n",
    "    return graph_type in (\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\",\"double_VG\") and classif in (\"graph\", \"node\") and len_type in (\"un/cut\", \"random\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37303612-ad33-4149-bb9c-f5c62aa27d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "            self.lr_find(trainer, pl_module)\n",
    "\n",
    "# class for node classification            \n",
    "class GINE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(GINE, self).__init__()\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"visual\"):\n",
    "            edge_dim = 1\n",
    "        elif graph_type in (\"MTF_on_visual\", \"visual_on_MTF\", \"double_VG\"):\n",
    "            edge_dim = 2\n",
    "            \n",
    "        dim_h = 32\n",
    "    \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "                     \n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device) #weight=class_weights\n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        correct=out.argmax(dim=1).eq(val_batch.y).sum().item()\n",
    "        total=len(val_batch.y)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        \n",
    "        logs={\"train_loss\": val_loss}\n",
    "        batch_dictionary={\"loss\": val_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        out = model(test_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_true\", test_label)\n",
    "        self.log(\"test_pred\", pred)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, test_label\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        #this function gives us in the outputs all acumulated pred and test_labels we returned in test_step\n",
    "        #we transform the pred and test_label into a shape that the classification report can read\n",
    "        true_array=[]\n",
    "        pred_array = []\n",
    "        for i in range(len(outputs)):\n",
    "            true_array = np.append(true_array,outputs[i][1].cpu().numpy())\n",
    "            pred_array = np.append(pred_array,outputs[i][0].cpu().numpy())            \n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array))\n",
    "        return pred_array, true_array\n",
    "    \n",
    "#---------------------------------------------------------------------------------\n",
    "# class for node classification\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(1, 32, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(1, 4 * 32)\n",
    "        self.conv2 = GATConv(4 * 32, 32, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 32, 4 * 32)\n",
    "        self.conv3 = GATConv(4 * 32, 1, heads=6,concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):        \n",
    "        out = model(train_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device) #weight=class_weights[1]\n",
    "        \n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        val_label = val_batch.y.cpu()\n",
    "        ys.append(val_batch.y)\n",
    "        preds.append((out > 0).float().cpu())     \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "    \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        test_label = test_batch.y.cpu()\n",
    "        ys.append(test_batch.y)\n",
    "        preds.append((out > 0).float().cpu())\n",
    "        \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        \n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, y\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        #this function gives us in the outputs all acumulated pred and test_labels we returned in test_step\n",
    "        #we transform the pred and test_label into a shape that the classification report can read\n",
    "        global true_array, pred_array\n",
    "        true_array=[outputs[i][1].cpu().numpy() for i in range(len(outputs))]\n",
    "        pred_array = [outputs[i][0].cpu().numpy() for i in range(len(outputs))]\n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array))\n",
    "        print(\"pred_array \",pred_array)\n",
    "\n",
    "        \n",
    "def main():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        global model, test_loader, device\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_acc',patience=patience, strict=False,verbose=False, mode='max')\n",
    "        val_checkpoint_acc = ModelCheckpoint(filename=\"max_acc-{epoch}-{step}-{val_acc:.3f}\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_best_loss = ModelCheckpoint(filename=\"best_loss\", monitor = \"val_loss\", mode=\"max\")\n",
    "        val_checkpoint_best_acc = ModelCheckpoint(filename=\"best_acc\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_loss = ModelCheckpoint(filename=\"min_loss-{epoch}-{step}-{val_loss:.3f}\", monitor = \"val_loss\", mode=\"min\")\n",
    "        latest_checkpoint = ModelCheckpoint(filename=\"latest-{epoch}-{step}\", monitor = \"step\", mode=\"max\",every_n_train_steps = 500,save_top_k = 1)\n",
    "        batchsizefinder = BatchSizeFinder(mode='power', steps_per_trial=3, init_val=2, max_trials=25, batch_arg_name='batch_size')\n",
    "        lr_finder = FineTuneLearningRateFinder(milestones=(5,10))\n",
    "        tb_logger = TensorBoardLogger(save_file, name=name_of_save) # where the model saves the callbacks\n",
    "        dvclive_logger = DVCLiveLogger()\n",
    "        \n",
    "        torch.manual_seed(SEED)\n",
    "        print(SEED)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        output = generate_output() # creates dataset with graphs\n",
    "        # my data        \n",
    "        \n",
    "        train_size = int(0.8 * len(output))\n",
    "        Temp_size = len(output) - train_size\n",
    "        val_size = int(0.2*Temp_size)\n",
    "        test_size = Temp_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "                \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # mode\n",
    "        if classif == \"graph\":\n",
    "            model = GINE().double()#.to(device) #GINE is for graph classification\n",
    "        elif classif == \"node\":\n",
    "            model = Net().double()#.to(device) #net is for node classification\n",
    "            \n",
    "        #training\n",
    "        \n",
    "        \n",
    "        trainer = pl.Trainer(logger=tb_logger,max_epochs = range_epoch, callbacks=[val_checkpoint_best_loss, val_checkpoint_best_acc, latest_checkpoint, val_checkpoint_acc,val_checkpoint_loss,early_stop],accelerator='gpu',devices=1)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "# if automaticly checks number of saves in folder and runs all of them in order\n",
    "\n",
    "def my_test_loop():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        import os, sys\n",
    "        versions = os.listdir(\"Models/\"+version)\n",
    "        len_ver = len(versions)\n",
    "        SEED_temp=SEED\n",
    "        for i in range(len_ver):\n",
    "\n",
    "            global device, model\n",
    "\n",
    "            torch.manual_seed(SEED_temp)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            output = generate_output() # creates dataset with graphs\n",
    "\n",
    "            train_size = int(0.8 * len(output))\n",
    "            Temp_size = len(output) - train_size\n",
    "            val_size = int(0.2*Temp_size)\n",
    "            test_size = Temp_size - val_size\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "            \n",
    "            if classif == \"graph\":\n",
    "                model = GINE.load_from_checkpoint(\"Models/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double()\n",
    "            elif classif == \"node\":\n",
    "                model = Net.load_from_checkpoint(\"Models/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double()\n",
    "            \n",
    "            trainer = pl.Trainer(accelerator='gpu',devices=1)\n",
    "            trainer.test(model, test_loader)\n",
    "            # checks if version is the same as seed(just to bo sure)\n",
    "            print(\"version_\"+str(i))\n",
    "            print(SEED_temp)\n",
    "            SEED_temp += 1\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "#main parametrs\n",
    "graph_type = \"visual\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", \"double_VG\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "#paths\n",
    "path_main = \"dataset_100_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "# params for \n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "learning_rate = 0.01\n",
    "batch_size = 64*2\n",
    "range_epoch = 500 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"100\"\n",
    "#-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b80ea36-dc24-4b43-826c-6657c845fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the anomaly type of node classification prediction\n",
    "def anomaly_type(my_model):\n",
    "    #I know that the program very often predicts, if model has all 300 samples, that the first 3 are anomalies, and that is not true so ...\n",
    "    if len(my_model) == 300:\n",
    "        my_model[0] = 0\n",
    "        my_model[1] = 0\n",
    "        my_model[2] = 0\n",
    "    \n",
    "    if len(my_model)%2 != 0:\n",
    "        my_model = np.append(0,my_model)\n",
    "    par1 = int(len(my_model)/2)\n",
    "    \n",
    "    #check if anywhere in array there is an anomaly, otherwise predict this is a No_anomaly\n",
    "    if any(my_model) == 1:\n",
    "        \n",
    "        mm1 = [all(i) for i in my_model.reshape(par1, -1)]\n",
    "        mm2 = [all(i) for i in my_model[1:-1].reshape(par1, -1)]\n",
    "        #check if anywhere in the array there are two 1 gruped together, otherwise predict this is a InstaD\n",
    "        if any(mm1) == 1 and any(mm2) == 1:\n",
    "        \n",
    "            #check if last in the array is 1 (this tells me if the anomaly ends or not), otherwise predict this is a SuddenR\n",
    "            if my_model[-1] == 1:\n",
    "                \n",
    "                mm3 = [all(i) for i in my_model.reshape(2, -1)]\n",
    "                #you can't differentiate SlowD from SuddenD other than that that SlowD starts very early in the array, so if second half of the array is all 1 predict Slow, otherwise predict this is a SuddenD\n",
    "                if any(mm3) == 1:\n",
    "                    \n",
    "                    return \"SlowD\"\n",
    "                return \"SuddenD\"\n",
    "            return \"SuddenR\"\n",
    "        return \"InstaD\"\n",
    "    return \"No_anomaly\"\n",
    "\n",
    "#defines where the anomalies are (where they start and end) and put it in a text\n",
    "def Anomaly_loc(pred_type,pred_pred):\n",
    "    W_list = []\n",
    "    for i in range(len(pred_type)):\n",
    "        \n",
    "        result = [j for j, x in enumerate(pred_model[i]) if x]\n",
    "        \n",
    "        if pred_type[i] == \"No_anomaly\":\n",
    "            W = \"There is no anomaly\"\n",
    "            \n",
    "        elif pred_type[i] == \"InstaD\":\n",
    "            result_str = [str(a) for a in result]\n",
    "            W = \"Anomaly type is Instant Degredation and the anomalies are located at times \"+', '.join(result_str)\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenR\":\n",
    "            W = \"Anomaly type is Sudden Recovery and the anomalies are located at times from \" + str(result[0]) + \" to \" + str (result[-1])\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenD\":\n",
    "            W = \"Anomaly type is Sudden Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        \n",
    "        elif pred_type[i] == \"SlowD\":\n",
    "            W = \"Anomaly type is Slow Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        W_list = np.append(W_list, W)\n",
    "\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c85c0-2072-47a3-9118-978470089691",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1da32-0d28-44ed-995c-392aeafa373c",
   "metadata": {},
   "source": [
    "**Seed numbers for every instance of dataset (they add up)**  \n",
    "MTF == 0  \n",
    "VG == 1000  \n",
    "\n",
    "MTF_on_VG == 2000  \n",
    "VG_on_MTF == 3000  \n",
    "\n",
    "graph == 0  \n",
    "node == 100  \n",
    "\n",
    "uncut == 0  \n",
    "random == 10  \n",
    "\n",
    "50 == 20  \n",
    "100 == 30  \n",
    "150 == 40  \n",
    "200 == 50  \n",
    "250 == 60  \n",
    "25 == 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ef442f-7719-4414-b905-5a0075fabe71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: '_Helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42/4106335263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mts2vg\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhelp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: '_Helper'"
     ]
    }
   ],
   "source": [
    "ts2vg --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd69380-e100-4676-a6da-428ba71c7688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "graph_type = \"MTF\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "range_epoch = 10000 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"MTF_2\"\n",
    "patience = 2000\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(1): \n",
    "    SEED = i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bea199-fce0-4135-a6ed-de7e721f2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"visual\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", graph topology\n",
    "classif = \"node\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"VG_node_classification_rounded\"\n",
    "patience = 500\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(3): \n",
    "    SEED = 1100+i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e53567-5995-4823-ad17-cd2fa6ebb650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph classification on un/cut time series using MTF graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | conv1 | GINEConv | 2.2 K \n",
      "1 | conv2 | GINEConv | 2.2 K \n",
      "2 | conv3 | GINEConv | 2.2 K \n",
      "3 | conv4 | GINEConv | 2.2 K \n",
      "4 | conv5 | GINEConv | 2.2 K \n",
      "5 | lin1  | Linear   | 25.8 K\n",
      "6 | lin2  | Linear   | 805   \n",
      "-----------------------------------\n",
      "37.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "37.8 K    Total params\n",
      "0.151     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6888cae514b34c79bb72ab402fb60b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_type = \"MTF\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "\n",
    "path_main = \"datasets/dataset_250_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 64*2\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"MTF_250\"\n",
    "patience = 300\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(1): \n",
    "    SEED = 60+i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7c375-f2b3-473f-a0d5-9c32eed6e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"MTF\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "\n",
    "path_main = \"datasets/dataset_25_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 64*2\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"MTF_25\"\n",
    "patience = 200\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(6): \n",
    "    SEED = 70+i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e43e7-2526-43b8-a5a4-bda0d5982714",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e549935-ef2c-4ecc-afa6-0013f39150ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"VG_50\"\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "graph_type = \"visual\" #\"visual\", \"MTF\", \"MTF_on_visual\", \"visual_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "path_main = \"datasets/dataset_50_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "SEED = 1020\n",
    "\n",
    "my_test_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
