{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0d00-7f18-4047-83e9-263316278bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install ts2vg\n",
    "!pip install pytorch_lightning\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import ts2vg\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pyts.image import MarkovTransitionField\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateFinder\n",
    "from pytorch_lightning.callbacks import BatchSizeFinder\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1507f595-2818-4106-bfa8-6c2b85d1668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for graph creation\n",
    "def create_MTF_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]#adj_mat[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123e5ab4-575b-4265-ac9d-99e6318a6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #function for graph creation\n",
    "def create_visual_graph():\n",
    "    # warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        \n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        # g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_visual = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_visual).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_visual[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        edge_index, edge_weight = adjToEdgidx(X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                           edge_index=torch.tensor(edge_index, dtype=torch.int64), \n",
    "                           edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),\n",
    "                           y=y_mask))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eca4cf2-1904-4169-afa1-4bbebbd9554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_join_graph():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "        \n",
    "    def adjToEdgidx(adj_mat_MTF,X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual = np.zeros([len(adj_mat_MTF),len(adj_mat_MTF)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_MTF).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual[row, col]\n",
    "        \n",
    "        # edge_weight = np.reshape(edge_weight,(len(edge_weight),2))\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j,X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels       \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                    edge_index=edge_index, \n",
    "                    edge_attr=torch.tensor(edge_weight, dtype=torch.double), \n",
    "                    y=y_mask))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4f32d8-da08-4baf-a8ab-56aba5f0ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Focal loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab96923-16af-4733-a746-8b9a47f7a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
    "\n",
    "    def __init__(self, gamma=1, alpha=None, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input_, target):\n",
    "        cross_entropy = super().forward(input_, target)\n",
    "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
    "        # This won't contribute final loss as the cross_entropy contribution\n",
    "        # for these would be zero.\n",
    "        target = target * (target != self.ignore_index).long()\n",
    "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
    "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
    "        \n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss) \n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss) \n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the last output so we dont have to generate another if all parameters are the same as before\n",
    "global temp_repeat\n",
    "temp_repeat=['']*6\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask:\n",
    "    \n",
    "        global output\n",
    "        if graph_type == \"MTF\":\n",
    "            output = create_MTF_graph()\n",
    "        if graph_type == \"visual\":\n",
    "            output = create_visual_graph()\n",
    "    \n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37303612-ad33-4149-bb9c-f5c62aa27d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of all Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_acc',patience=300, strict=False,verbose=False, mode='max')\n",
    "model_chackpoint = ModelCheckpoint(monitor='val_loss', mode='min')\n",
    "val_checkpoint_acc = ModelCheckpoint(filename=\"max_acc-{epoch}-{step}-{val_loss:.1f}\", monitor = \"val_acc\", mode=\"max\")\n",
    "val_checkpoint_best = ModelCheckpoint(filename=\"best\", monitor = \"val_acc\", mode=\"max\")\n",
    "val_checkpoint_loss = ModelCheckpoint(filename=\"min_loss-{epoch}-{step}-{val_loss:.1f}\", monitor = \"val_loss\", mode=\"min\")\n",
    "latest_checkpoint = ModelCheckpoint(filename=\"latest-{epoch}-{step}\", monitor = \"step\", mode=\"max\",every_n_train_steps = 500,save_top_k = 1)\n",
    "batchsizefinder = BatchSizeFinder(mode='power', steps_per_trial=3, init_val=2, max_trials=25, batch_arg_name='batch_size')\n",
    "CSV_logger = CSVLogger(\"logs\", name=\"my_exp_name\",flush_logs_every_n_steps=1)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "            self.lr_find(trainer, pl_module)\n",
    "\n",
    "lr_finder = FineTuneLearningRateFinder(milestones=(5,10))\n",
    "\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    #Layers\n",
    "    def __init__(self):\n",
    "        super(LitAutoEncoder, self).__init__()\n",
    "        edge_dim = 1\n",
    "        dim_h = 32\n",
    "        \n",
    "        \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "                     \n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device) #weight=class_weights\n",
    "        \n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        correct=out.argmax(dim=1).eq(val_batch.y).sum().item()\n",
    "        total=len(val_batch.y)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        \n",
    "        logs={\"train_loss\": val_loss}\n",
    "        batch_dictionary={\"loss\": val_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        # class_weights.type_as(\"gpu\")\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_true\", test_label)\n",
    "        self.log(\"test_pred\", pred)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, test_label\n",
    "    \n",
    "    #test_epoch_end, lets us gather all outputs of test_step so we can show the classificaiton_report\n",
    "    def test_epoch_end(self, outputs):\n",
    "        true_array=[]\n",
    "        pred_array = []\n",
    "        for i in range(len(outputs)):\n",
    "            true_array = np.append(true_array,outputs[i][1].cpu().numpy())\n",
    "            pred_array = np.append(pred_array,outputs[i][0].cpu().numpy())            \n",
    "        \n",
    "        print(classification_report(true_array, pred_array))\n",
    "        \n",
    "#Definition of all global veriables----------------------------\n",
    "\n",
    "graph_type = \"visual\"#\"visual\", \"MTF\"\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", set the shape of data used in later paths \n",
    "path_main = \"dataset_uncut.csv\" # \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "batch_size = 64*4 #set the train batch size\n",
    "SEED = 301\n",
    "learning_rate = 0.01\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"tb_logs\"\n",
    "name_of_save = \"len=300_5_models\"\n",
    "logger = TensorBoardLogger(save_file, name=name_of_save) # where the model saves the callbacks for tensorBoard\n",
    "    \n",
    "\n",
    "\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "output = generate_output()\n",
    "# my data        \n",
    "    \n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_size = int(0.8 * len(output))\n",
    "Temp_size = len(output) - train_size\n",
    "val_size = int(0.2*Temp_size)\n",
    "test_size = Temp_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model\n",
    "model = LitAutoEncoder().double()#.to(device)\n",
    "\n",
    "# #training\n",
    "val_check_interval=len(train_loader)\n",
    "\n",
    "#lr finder\n",
    "trainer = pl.Trainer(auto_lr_find=True,accelerator='gpu',devices=1)\n",
    "lr_finder = trainer.tuner.lr_find(model, train_loader)\n",
    "fig = lr_finder.plot(suggest = True)\n",
    "\n",
    "#running the training and validation\n",
    "trainer = pl.Trainer(logger=logger,max_epochs = 1000, callbacks=[latest_checkpoint, val_checkpoint_acc,val_checkpoint_loss,lr_finder,early_stop],accelerator='gpu',devices=1)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "#chose a model from checkpoint file in designated location in logger or use the generated model defined above and comment the next line\n",
    "model = LitAutoEncoder.load_from_checkpoint(save_file +\"/\"+name_of_save +\"/version_0/checkpoints/max_acc-epoch=289-step=9792-val_loss=0.0.ckpt\").double()\n",
    "\n",
    "#run the test to get classification_report\n",
    "trainer.test(model, test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
