{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71e8e40-5bd1-4cdc-ac0e-0ac3088277ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython #==0.29.24\n",
    "!pip install numba #==0.56.2\n",
    "!pip install pandas #==1.3.4\n",
    "!pip install networkx #==2.6.3\n",
    "!pip install matplotlib #==3.4.3\n",
    "!pip install ts2vg #==1.0.0\n",
    "!pip install pytorch_lightning==1.6.5\n",
    "!pip install tensorflow #==2.11.0\n",
    "!pip install dvclive #==1.3.3\n",
    "!pip install torchsummary #==1.5.1\n",
    "!pip install torchmetrics==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f0918-2029-4ad3-83cb-da419b40e97d",
   "metadata": {},
   "source": [
    "# pip from 31/1 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9141d-573a-4e7e-b6f1-8ea03ec4ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch-cuda=11.6 -c pytorch -c conda-forge -c nvidia -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c6da4-b2cf-43fb-9b1b-3153362900b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60075bae-4638-495d-b2de-bb51ad329697",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.1+cu116.html -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2a781-fb57-4225-9142-f6aa89e02583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ts2vg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96a186-123d-455e-8d51-643c08ddce24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning==1.6.5 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613209aa-d3bb-4001-ad9f-95c6b34c8a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mamba install tensorflow-gpu -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b527e-57d0-4061-83c8-6abb27dedfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y #==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9065bc1-0ce8-4d16-9c34-7821651cee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64d07e-430e-43fd-8fd7-11a12531e4b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bellow are listed all nedded libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import ts2vg\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from pyts.image import MarkovTransitionField\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import ProgressBarBase\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "#from pytorch_lightning.loggers import CSVLogger\n",
    "#from dvclive.lightning import DVCLiveLogger\n",
    "\n",
    "#from torchsummary import summary\n",
    "\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429b1b52-626e-4eb9-886a-e6f050c23778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "in this bracket of code is located the conversion of time series to different graph neural networks (Markov transition field (MTF) and Visibility graphs (VG)) \n",
    "with different lengths (uncut(300), cut and random(where every sample has a different length between 300 and 50)).\n",
    "\n",
    "create graph is called when we have a time series that needs to be converted into a graph. I\n",
    "\n",
    "\"\"\"\n",
    "def create_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy() # x values for every sample\n",
    "        #X = np.round_(X,1)\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8) # types of anomalies\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy() # binary location of anomalies\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "            MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "            X_gaf_MTF = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            dataset_properties[i,1] = int(dataset_properties[i,1])\n",
    "        \n",
    "        X = dataset_rss # x values for every sample\n",
    "        X_mask = dataset_mask # binary location of anomalies\n",
    "        Y = dataset_properties[:,2] # types of anomalies\n",
    "        Y_len = dataset_properties[:,0] # length of every sample\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis_on_MTF\", \"MTF_on_vis\"):\n",
    "            X_gaf_MTF = []\n",
    "            for i in range(len(Y_len)):\n",
    "                MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "                X_gaf_MTF_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "                X_gaf_MTF.append(X_gaf_MTF_temp[0])\n",
    "    \n",
    "    # output will contain all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    def vis_matrix(X_current, vis_type_local):\n",
    "        if vis_type_local == \"natural\":\n",
    "            g = NaturalVG(weighted=vis_distance)\n",
    "        elif vis_type_local == \"horizontal\":\n",
    "            g = HorizontalVG(weighted=vis_distance)\n",
    "            \n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_vis = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_vis[x,y] = q\n",
    "            if vis_edge_type == \"undirected\":\n",
    "                adj_mat_vis[y,x] = q\n",
    "        \n",
    "        return adj_mat_vis\n",
    "    \n",
    "    # functions for creating edge index and edge weight for a given matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        #function for visibility and MTF matrixes\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]\n",
    "        return edge_index,edge_weight\n",
    "    \n",
    "    def adjToEdgidx_dual_VG(X_current):\n",
    "        #function for joined visibility and MTF matrixes\n",
    "        pos_adj_mat_vis = vis_matrix(X_current, vis_type)\n",
    "        neg_adj_mat_vis = vis_matrix(-X_current, vis_type)\n",
    "            \n",
    "        edge_index = torch.from_numpy(pos_adj_mat_vis+neg_adj_mat_vis).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays\n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = pos_adj_mat_vis[row, col]\n",
    "        edge_weight[:,1] = neg_adj_mat_vis[row, col]\n",
    "        return edge_index, edge_weight\n",
    "        \n",
    "    def define_mask(i):\n",
    "        if classif == \"graph\": # for graph classification\n",
    "            return torch.tensor(Y[i], dtype=torch.long)\n",
    "        elif classif == \"node\":# for node classification \n",
    "            return torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "    \n",
    "    if graph_type == \"MTF\":  \n",
    "        for i, j in enumerate(X_gaf_MTF):\n",
    "            edge_index, edge_weight = adjToEdgidx(j)\n",
    "            y_mask = define_mask(i)\n",
    "            #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally label\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    elif graph_type == \"vis\":\n",
    "        for i in range(len(X)):\n",
    "            edge_index, edge_weight = adjToEdgidx(vis_matrix(X[i],vis_type))\n",
    "            y_mask = define_mask(i)\n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=torch.tensor(edge_index, dtype=torch.int64), edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),y=y_mask))    \n",
    "    \n",
    "    elif graph_type == \"dual_VG\":\n",
    "        for i in range(len(X)):\n",
    "            edge_index, edge_weight = adjToEdgidx_dual_VG(X[i])\n",
    "            y_mask = define_mask(i) \n",
    "            output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.tensor(edge_weight, dtype=torch.double), y=y_mask))    \n",
    "    \n",
    "    return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37303612-ad33-4149-bb9c-f5c62aa27d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in this function we use GINE for graph classification and Net for node classidication using Pytorch Lightning for the graphs we created by using the create_graph function.\n",
    "This function also incorporates callbacks for easier and faster result agregation.\n",
    "\"\"\"\n",
    "\n",
    "# class for node classification            \n",
    "class GINE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(GINE, self).__init__()\n",
    "        \n",
    "        if graph_type in (\"MTF\", \"vis\"):\n",
    "            edge_dim = 1\n",
    "        elif graph_type in (\"MTF_on_vis\", \"vis_on_MTF\", \"double_VG\", \"dual_VG\"):\n",
    "            edge_dim = 2\n",
    "            \n",
    "        dim_h = 32\n",
    "    \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "    \n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "                     \n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        correct=out.argmax(dim=1).eq(val_batch.y).sum().item()\n",
    "        total=len(val_batch.y)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        \n",
    "        logs={\"train_loss\": val_loss}\n",
    "        batch_dictionary={\"loss\": val_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        out = model(test_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_true\", test_label)\n",
    "        self.log(\"test_pred\", pred)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, test_label\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        #this function gives us in the outputs all acumulated pred and test_labels we returned in test_step\n",
    "        #we transform the pred and test_label into a shape that the classification report can read\n",
    "        true_array=[]\n",
    "        pred_array = []\n",
    "        for i in range(len(outputs)):\n",
    "            true_array = np.append(true_array,outputs[i][1].cpu().numpy())\n",
    "            pred_array = np.append(pred_array,outputs[i][0].cpu().numpy())            \n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array))\n",
    "        return pred_array, true_array\n",
    "    \n",
    "#---------------------------------------------------------------------------------\n",
    "# class for node classification\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(1, 32, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(1, 4 * 32)\n",
    "        self.conv2 = GATConv(4 * 32, 32, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 32, 4 * 32)\n",
    "        self.conv3 = GATConv(4 * 32, 1, heads=6,concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):        \n",
    "        out = model(train_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device) #weight=class_weights\n",
    "        \n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        val_label = val_batch.y.cpu()\n",
    "        ys.append(val_batch.y)\n",
    "        preds.append((out > 0).float().cpu())     \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "    \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        loss_function = BCEWithLogitsLoss().to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        ys, preds = [], []\n",
    "        test_label = test_batch.y.cpu()\n",
    "        ys.append(test_batch.y)\n",
    "        preds.append((out > 0).float().cpu())\n",
    "        \n",
    "        y, pred = torch.cat(ys, dim=0), torch.cat(preds, dim=0)\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        \n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        return pred, y\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        #this function gives us in the outputs all acumulated pred and test_labels we returned in test_step\n",
    "        #we transform the pred and test_label into a shape that the classification report can read\n",
    "        global true_array, pred_array\n",
    "        true_array=[outputs[i][1].cpu().numpy() for i in range(len(outputs))]\n",
    "        pred_array = [outputs[i][0].cpu().numpy() for i in range(len(outputs))]\n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array))\n",
    "        print(\"pred_array \",pred_array)\n",
    "\n",
    "        \n",
    "def main():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        global model, test_loader, device\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_acc',patience=patience, strict=False,verbose=False, mode='max')\n",
    "        val_checkpoint_acc = ModelCheckpoint(filename=\"max_acc-{epoch}-{step}-{val_acc:.3f}\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_best_loss = ModelCheckpoint(filename=\"best_loss\", monitor = \"val_loss\", mode=\"max\")\n",
    "        val_checkpoint_best_acc = ModelCheckpoint(filename=\"best_acc\", monitor = \"val_acc\", mode=\"max\")\n",
    "        val_checkpoint_loss = ModelCheckpoint(filename=\"min_loss-{epoch}-{step}-{val_loss:.3f}\", monitor = \"val_loss\", mode=\"min\")\n",
    "        latest_checkpoint = ModelCheckpoint(filename=\"latest-{epoch}-{step}\", monitor = \"step\", mode=\"max\",every_n_train_steps = 500,save_top_k = 1)\n",
    "        #batchsizefinder = BatchSizeFinder(mode='power', steps_per_trial=3, init_val=2, max_trials=25, batch_arg_name='batch_size')\n",
    "        #lr_finder = FineTuneLearningRateFinder(milestones=(5,10))\n",
    "        tb_logger = TensorBoardLogger(save_file, name=name_of_save) # where the model saves the callbacks\n",
    "        #dvclive_logger = DVCLiveLogger()\n",
    "        \n",
    "        torch.manual_seed(SEED)\n",
    "        print(SEED)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        output = generate_output() # creates dataset with graphs  \n",
    "        \n",
    "        train_size = int(0.8 * len(output))\n",
    "        Temp_size = len(output) - train_size\n",
    "        val_size = int(0.2*Temp_size)\n",
    "        test_size = Temp_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "                \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=int(batch_size/2), shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # mode\n",
    "        if classif == \"graph\":\n",
    "            model = GINE().double()#.to(device) #GINE is for graph classification\n",
    "        elif classif == \"node\":\n",
    "            model = Net().double()#.to(device) #net is for node classification\n",
    "            \n",
    "        #training\n",
    "        \n",
    "        trainer = pl.Trainer(logger=tb_logger, max_epochs = range_epoch, callbacks=[val_checkpoint_best_loss, val_checkpoint_best_acc, latest_checkpoint, val_checkpoint_acc,val_checkpoint_loss,early_stop],accelerator='gpu',devices=1)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        \n",
    "# it automaticly checks number of saves in folder and runs all of them in order\n",
    "def my_test_loop():\n",
    "    if check_for_missclick() != True:\n",
    "        print(\"one of the main parameters does not match\")\n",
    "    else:\n",
    "        import os, sys\n",
    "        versions = os.listdir(\"Models/\"+version)\n",
    "        len_ver = len(versions)\n",
    "        SEED_temp = SEED\n",
    "        for i in range(len_ver):\n",
    "\n",
    "            global device, model\n",
    "\n",
    "            torch.manual_seed(SEED_temp)\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            output = generate_output() # creates dataset with graphs\n",
    "\n",
    "            train_size = int(0.8 * len(output))\n",
    "            Temp_size = len(output) - train_size\n",
    "            val_size = int(0.2*Temp_size)\n",
    "            test_size = Temp_size - val_size\n",
    "            train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "            \n",
    "            if classif == \"graph\":\n",
    "                model = GINE.load_from_checkpoint(\"Models/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double()\n",
    "            elif classif == \"node\":\n",
    "                model = Net.load_from_checkpoint(\"Models/\"+version+\"/version_\" + str(i) + \"/checkpoints/\" + monitor_test).double()\n",
    "            \n",
    "            trainer = pl.Trainer(accelerator='gpu',devices=1)\n",
    "            trainer.test(model, test_loader)\n",
    "            # checks if version is the same as seed(just to bo sure)\n",
    "            print(\"version_\"+str(i))\n",
    "            print(SEED_temp)\n",
    "            SEED_temp += 1\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "#main parametrs\n",
    "graph_type = \"vis\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "cut_len = '0' # 0 if we arent doing cut by other lengths than 300 or random \n",
    "vis_type = \"natural\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "#paths\n",
    "path_main = \"dataset_100_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "# params for \n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "learning_rate = 0.01\n",
    "batch_size = 64*2\n",
    "range_epoch = 500 #set length of epoch\n",
    "save_file=\"Models\"\n",
    "name_of_save = \"100\"\n",
    "#-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b80ea36-dc24-4b43-826c-6657c845fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the anomaly type of node classification prediction\n",
    "def anomaly_type(my_model):\n",
    "    #I know that the program very often predicts, if model has all 300 samples, that the first 3 are anomalies, and that is not true so ...\n",
    "    if len(my_model) == 300:\n",
    "        my_model[0] = 0\n",
    "        my_model[1] = 0\n",
    "        my_model[2] = 0\n",
    "    \n",
    "    if len(my_model)%2 != 0:\n",
    "        my_model = np.append(0,my_model)\n",
    "    par1 = int(len(my_model)/2)\n",
    "    \n",
    "    #check if anywhere in array there is an anomaly, otherwise predict this is a No_anomaly\n",
    "    if any(my_model) == 1:\n",
    "        \n",
    "        mm1 = [all(i) for i in my_model.reshape(par1, -1)]\n",
    "        mm2 = [all(i) for i in my_model[1:-1].reshape(par1, -1)]\n",
    "        #check if anywhere in the array there are two 1 gruped together, otherwise predict this is a InstaD\n",
    "        if any(mm1) == 1 and any(mm2) == 1:\n",
    "        \n",
    "            #check if last in the array is 1 (this tells me if the anomaly ends or not), otherwise predict this is a SuddenR\n",
    "            if my_model[-1] == 1:\n",
    "                \n",
    "                mm3 = [all(i) for i in my_model.reshape(2, -1)]\n",
    "                #you can't differentiate SlowD from SuddenD other than that that SlowD starts very early in the array, so if second half of the array is all 1 predict Slow, otherwise predict this is a SuddenD\n",
    "                if any(mm3) == 1:\n",
    "                    \n",
    "                    return \"SlowD\"\n",
    "                return \"SuddenD\"\n",
    "            return \"SuddenR\"\n",
    "        return \"InstaD\"\n",
    "    return \"No_anomaly\"\n",
    "\n",
    "#defines where the anomalies are (where they start and end) and put it in a text\n",
    "def Anomaly_loc(pred_type,pred_pred):\n",
    "    W_list = []\n",
    "    for i in range(len(pred_type)):\n",
    "        \n",
    "        result = [j for j, x in enumerate(pred_model[i]) if x]\n",
    "        \n",
    "        if pred_type[i] == \"No_anomaly\":\n",
    "            W = \"There is no anomaly\"\n",
    "            \n",
    "        elif pred_type[i] == \"InstaD\":\n",
    "            result_str = [str(a) for a in result]\n",
    "            W = \"Anomaly type is Instant Degredation and the anomalies are located at times \"+', '.join(result_str)\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenR\":\n",
    "            W = \"Anomaly type is Sudden Recovery and the anomalies are located at times from \" + str(result[0]) + \" to \" + str (result[-1])\n",
    "            \n",
    "        elif pred_type[i] == \"SuddenD\":\n",
    "            W = \"Anomaly type is Sudden Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        \n",
    "        elif pred_type[i] == \"SlowD\":\n",
    "            W = \"Anomaly type is Slow Degredation and the anomalies are located at times from \" + str(result[0]) + \" and do not stop\"\n",
    "        W_list = np.append(W_list, W)\n",
    "\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this definition keeps the last output so we dont have to generate another if all parameters are the same as before, usefull for when you change the modell a lot but not the graph itself\n",
    "\"\"\"\n",
    "global temp_repeat\n",
    "#we have 9 parameters to remember\n",
    "temp_repeat=['']*9\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask or temp_repeat[6] != vis_type or temp_repeat[7] != vis_distance or temp_repeat[8] != vis_edge_type:\n",
    "    \n",
    "        global output\n",
    "        output = create_graph()\n",
    "        \n",
    "    #remembering the last known parameter\n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    temp_repeat[6] = vis_type\n",
    "    temp_repeat[7] = vis_distance\n",
    "    temp_repeat[8] = vis_edge_type\n",
    "    \n",
    "    print(temp_repeat[1] + \" classification on \" + temp_repeat[2] + \" time series using \" + temp_repeat[0] + \" graphs\" )\n",
    "    return output\n",
    "#if we write a parameter that is not known to the function this def will stop it faster beffore you get an error half through the graph generation \n",
    "def check_for_missclick():\n",
    "    return graph_type in (\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\",\"double_VG\",\"dual_VG\") and classif in (\"graph\", \"node\") and len_type in (\"un/cut\", \"random\") and vis_type in (\"natural\", \"horizontal\") and vis_distance in ('slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance') and vis_edge_type in (\"undirected\", \"directed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c85c0-2072-47a3-9118-978470089691",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc76436-aa6c-4844-89a0-ab20511f2660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTF  :  0\n",
      "vis  :  1000\n",
      "MTF_on_VG  :  2000\n",
      "VG_on_MTF  :  3000\n",
      "dual_VG  :  0\n",
      "graph  :  0\n",
      "node  :  100\n",
      "un/cut  :  0\n",
      "random  :  10\n",
      "0  :  0\n",
      "50  :  20\n",
      "100  :  30\n",
      "150  :  40\n",
      "200  :  50\n",
      "250  :  60\n",
      "25  :  70\n",
      "natural  :  0\n",
      "horizontal  :  4000\n",
      "distance  :  0\n",
      "v_distance  :  100000\n",
      "abs_v_distance  :  200000\n",
      "slope  :  300000\n",
      "abs_slope  :  400000\n",
      "undirected  :  0\n",
      "directed  :  5000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Seed numbers for every instance of dataset (they add up (VG + graph + random))\n",
    "\"\"\"\n",
    "\n",
    "Dict = {\n",
    "    #graph_type\n",
    "        \"MTF\" : 0,  \n",
    "        \"vis\" : 1000, \n",
    "        \"MTF_on_VG\" : 2000,  \n",
    "        \"VG_on_MTF\" : 3000,\n",
    "        \"dual_VG\" : 0,\n",
    "    #classif\n",
    "        \"graph\" : 0,  \n",
    "        \"node\" : 100,  \n",
    "    #len_type\n",
    "        \"un/cut\" : 0,  \n",
    "        \"random\" : 10,  \n",
    "    #cut_len\n",
    "        \"0\" : 0,  \n",
    "        \"50\" : 20,  \n",
    "        \"100\" : 30,  \n",
    "        \"150\" : 40,  \n",
    "        \"200\" : 50,  \n",
    "        \"250\" : 60,  \n",
    "        \"25\" : 70,\n",
    "    #parameters for visibility\n",
    "        \"natural\" : 0,  \n",
    "        \"horizontal\" : 4000,  \n",
    "    \n",
    "        \"distance\" : 0,\n",
    "        \"v_distance\" : 100000,  \n",
    "        \"abs_v_distance\" : 200000,  \n",
    "        \"slope\" : 300000,\n",
    "        \"abs_slope\" : 400000,\n",
    "        \n",
    "        \"undirected\":0,\n",
    "        \"directed\" : 5000\n",
    "}\n",
    "\n",
    "for key, value in Dict.items():\n",
    "    print(key, ' : ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd569c54-f2ff-4eef-bd5e-46261f6402ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"vis\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", \"dual_VG\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "cut_len = '0' # 0 if we arent doing cut by other lengths than 300 or random \n",
    "vis_type = \"natural\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "SEED = int(Dict[graph_type]+Dict[classif]+Dict[len_type]+Dict[cut_len]+Dict[vis_distance]+Dict[vis_type]+Dict[vis_edge_type])\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"test\"\n",
    "name_of_save = \"VG_dual_v_abs_compact_\" + str(SEED)\n",
    "patience = 400\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(1): \n",
    "    SEED += i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a380a6-af13-4823-9383-ec1bf82975da",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"dual_VG\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", \"dual_VG\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "cut_len = '0' # 0 if we arent doing cut by other lengths than 300 or random \n",
    "vis_type = \"horizontal\" #\"natural\", \"horizontal\"\n",
    "vis_distance = 'abs_v_distance'#'slope', 'abs_slope','distance','h_distance','v_distance','abs_v_distance',\n",
    "vis_edge_type = \"directed\" #\"undirected\", \"directed\"\n",
    "\n",
    "\n",
    "path_main = \"datasets/dataset_uncut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"datasets/dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"datasets/dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "SEED = int(Dict[graph_type]+Dict[classif]+Dict[len_type]+Dict[cut_len]+Dict[vis_distance]+Dict[vis_type]+Dict[vis_edge_type])\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "range_epoch = 1000 #set length of epoch\n",
    "save_file=\"Models_not_faulty\"\n",
    "name_of_save = \"HVG_dual_v_abs_\" + str(SEED)\n",
    "patience = 400\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "for i in range(5): \n",
    "    SEED += i\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e43e7-2526-43b8-a5a4-bda0d5982714",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e549935-ef2c-4ecc-afa6-0013f39150ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"VG_50\"\n",
    "monitor_test = \"best_acc.ckpt\"\n",
    "graph_type = \"vis\" #\"vis\", \"MTF\", \"MTF_on_vis\", \"vis_on_MTF\", graph topology\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "path_main = \"datasets/dataset_50_cut.csv\" #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "SEED = 1020\n",
    "\n",
    "my_test_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
