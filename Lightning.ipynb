{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d0d00-7f18-4047-83e9-263316278bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install pytorch=1.12 torchvision torchaudio cudatoolkit=11.3 -c pytorch -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82c180-a189-4cb9-8375-690bfd8d522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209013ec-4327-45ed-9038-815a6e93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -c conda-forge pyts -q -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a100-20d7-4bfc-bd15-a9e6c680bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llvmpy\n",
    "!pip install cython\n",
    "!pip install numba\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install ts2vg\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca3d551-0aab-49a3-8fb9-2b5632323daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pyts.image import MarkovTransitionField\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from torch.nn import Linear, CrossEntropyLoss\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, ChebConv, global_sort_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GINEConv, GATv2Conv, GATConv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import ts2vg\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1507f595-2818-4106-bfa8-6c2b85d1668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for graph creation\n",
    "def create_MTF_graph():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(adj_mat):\n",
    "        edge_index = torch.from_numpy(adj_mat).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat[row, col]#adj_mat[row, col]\n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j)\n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), edge_index=edge_index, edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1), y=y_mask))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123e5ab4-575b-4265-ac9d-99e6318a6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #function for graph creation\n",
    "def create_visual_graph():\n",
    "    # warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        \n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    # setting class_weights for graph\n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "    # function for creating edge index and edge weight for a given MTF matrix\n",
    "    def adjToEdgidx(X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        # g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_visual = np.zeros([len(X_current),len(X_current)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_visual).nonzero().t().contiguous()\n",
    "        row, col = edge_index\n",
    "        edge_weight = adj_mat_visual[row, col]\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        edge_index, edge_weight = adjToEdgidx(X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "        \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                           edge_index=torch.tensor(edge_index, dtype=torch.int64), \n",
    "                           edge_attr=torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double),1),\n",
    "                           y=y_mask))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eca4cf2-1904-4169-afa1-4bbebbd9554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_join_graph():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # preparation for un/cut graphs\n",
    "    if len_type == \"un/cut\":\n",
    "    \n",
    "        df = pd.read_csv(path_main)  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        X_mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "        MTF = MarkovTransitionField(n_bins=length_rss)\n",
    "        X_gaf = MTF.fit_transform(X)\n",
    "        \n",
    "    # preparation for random graphs\n",
    "    elif len_type == \"random\":\n",
    "        dataset_rss = np.load(path_main, allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(path_properties, allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(path_mask, allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            if  dataset_properties[i,1] == True:\n",
    "                dataset_properties[i,1] = 1\n",
    "            else:\n",
    "                dataset_properties[i,1] = 0\n",
    "        \n",
    "        X = dataset_rss\n",
    "        X_mask = dataset_mask\n",
    "        Y = dataset_properties[:,2]\n",
    "        Y_len = dataset_properties[:,0]\n",
    "\n",
    "        X_gaf = []\n",
    "        for i in range(len(Y_len)):\n",
    "            \n",
    "            MTF = MarkovTransitionField(n_bins=Y_len[i])\n",
    "            X_gaf_temp = MTF.fit_transform(X[i].reshape(1, -1))\n",
    "            X_gaf.append(X_gaf_temp[0])\n",
    "    \n",
    "    # output will have all graphs \n",
    "    output = []\n",
    "    \n",
    "    global class_weights\n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                   classes=np.unique(Y),\n",
    "                                                                   y=Y))\n",
    "        \n",
    "    def adjToEdgidx(adj_mat_MTF,X_current):\n",
    "        g = NaturalVG(weighted='distance')\n",
    "        #g = HorizontalVG(weighted='distance')\n",
    "        g.build(X_current)\n",
    "        \n",
    "        #create matrix for visual\n",
    "        adj_mat_visual = np.zeros([len(adj_mat_MTF),len(adj_mat_MTF)], dtype='float')\n",
    "        for i in range(len(g.edges)):\n",
    "            x, y, q =g.edges[i]\n",
    "            adj_mat_visual[x,y] = q#/g.weights.max()\n",
    "            adj_mat_visual[y,x] = q#/g.weights.max()\n",
    "        \n",
    "        edge_index = torch.from_numpy(adj_mat_MTF).nonzero().t().contiguous()\n",
    "        \n",
    "        #join two edge_weight arrays (visual is converted to fit MTF) \n",
    "        row, col = edge_index\n",
    "        edge_weight = np.zeros([len(row),2], dtype='float')\n",
    "        edge_weight[:,0] = adj_mat_MTF[row, col]\n",
    "        edge_weight[:,1] = adj_mat_visual[row, col]\n",
    "        \n",
    "        # edge_weight = np.reshape(edge_weight,(len(edge_weight),2))\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    for i, j in enumerate(X_gaf):\n",
    "        edge_index, edge_weight = adjToEdgidx(j,X[i])\n",
    "        \n",
    "        if classif == \"graph\": # for graph classification\n",
    "            y_mask = torch.tensor(Y[i], dtype=torch.long)      \n",
    "        elif classif == \"node\":                 # for node classification \n",
    "            y_mask = torch.unsqueeze(torch.tensor(X_mask[i], dtype=torch.double),1)\n",
    "            \n",
    "        #Into Data save node values \"x\", edge index from adjacency matrix and edge features/attributes, finally labels       \n",
    "        output.append(Data(x=torch.unsqueeze(torch.tensor(X[i], dtype=torch.double),1), \n",
    "                    edge_index=edge_index, \n",
    "                    edge_attr=torch.tensor(edge_weight, dtype=torch.double), \n",
    "                    y=y_mask))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4f32d8-da08-4baf-a8ab-56aba5f0ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Focal loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab96923-16af-4733-a746-8b9a47f7a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
    "\n",
    "    def __init__(self, gamma=1, alpha=None, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input_, target):\n",
    "        cross_entropy = super().forward(input_, target)\n",
    "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
    "        # This won't contribute final loss as the cross_entropy contribution\n",
    "        # for these would be zero.\n",
    "        target = target * (target != self.ignore_index).long()\n",
    "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
    "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
    "        \n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss) \n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss) \n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22144ec-0e8b-4310-a0df-0a2d41af56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the last output so we dont have to generate another if all parameters are the same as beffore\n",
    "global temp_repeat\n",
    "temp_repeat=['']*6\n",
    "def generate_output():\n",
    "    if temp_repeat[0] != graph_type or temp_repeat[1] != classif or temp_repeat[2] != len_type or temp_repeat[3] != path_main or temp_repeat[4] != path_properties or temp_repeat[5] != path_mask:\n",
    "    \n",
    "        global output\n",
    "        if graph_type == \"MTF\":\n",
    "            output = create_MTF_graph()\n",
    "        if graph_type == \"visual\":\n",
    "            output = create_visual_graph()\n",
    "    \n",
    "    temp_repeat[0] = graph_type \n",
    "    temp_repeat[1] = classif\n",
    "    temp_repeat[2] = len_type\n",
    "    temp_repeat[3] = path_main\n",
    "    temp_repeat[4] = path_properties\n",
    "    temp_repeat[5] = path_mask\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4e3cda64-29f2-4739-a877-ddf5526a9292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | conv1 | GINEConv | 2.2 K \n",
      "1 | conv2 | GINEConv | 2.2 K \n",
      "2 | conv3 | GINEConv | 2.2 K \n",
      "3 | conv4 | GINEConv | 2.2 K \n",
      "4 | conv5 | GINEConv | 2.2 K \n",
      "5 | lin1  | Linear   | 25.8 K\n",
      "6 | lin2  | Linear   | 805   \n",
      "-----------------------------------\n",
      "37.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "37.8 K    Total params\n",
      "0.151     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddfe46028a747f883e2ce7736523195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273e87e25369495eaabb65acf88fa4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Restoring states from the checkpoint path at saves/.lr_find_5a770113-0a8a-4e19-982b-5f242854ec7e.ckpt\n",
      "Restored all states from the checkpoint file at saves/.lr_find_5a770113-0a8a-4e19-982b-5f242854ec7e.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateFinder\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_acc',patience=10,strict=False,verbose=False, mode='max')\n",
    "model_chackpoint = ModelCheckpoint(monitor='val_loss', mode='min')\n",
    "val_checkpoint = ModelCheckpoint(filename=\"{epoch}-{step}-{val_loss:.1f}\", monitor = \"val_loss\", mode=\"min\")\n",
    "latest_checkpoint = ModelCheckpoint(filename=\"latest-{epoch}-{step}\", monitor = \"step\", mode=\"max\",every_n_train_steps = 500,save_top_k = 1)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "            self.lr_find(trainer, pl_module)\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LitAutoEncoder, self).__init__()\n",
    "        edge_dim = 1\n",
    "        dim_h = 32\n",
    "        \n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "        \n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "        \n",
    "        # Graph-level readout\n",
    "        \n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "        \n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "                     \n",
    "        out = model(train_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device) #weight=class_weights\n",
    "        \n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        correct=out.argmax(dim=1).eq(train_batch.y).sum().item()\n",
    "        logs={\"train_loss\": train_loss}\n",
    "        total=len(train_batch.y)\n",
    "        \n",
    "        batch_dictionary={\"loss\": train_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "      \n",
    "        out = model(val_batch)\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        correct=out.argmax(dim=1).eq(val_batch.y).sum().item()\n",
    "        total=len(val_batch.y)\n",
    "        val_label = val_batch.y\n",
    "        accuracy = (pred == val_label).sum() / pred.shape[0]\n",
    "        \n",
    "        logs={\"val_loss\": val_loss}\n",
    "        batch_dictionary={\"loss\": val_loss, \"log\": logs, \"correct\": correct, \"total\": total}\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        out = model(test_batch)\n",
    "        # class_weights.type_as(\"gpu\")\n",
    "        loss_function = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "        \n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        accuracy = (pred == test_label).sum() / pred.shape[0]\n",
    "        self.log(\"test_loss\", test_label)\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------\n",
    "graph_type = \"visual\"#\"visual\", \"MTF\"\n",
    "classif = \"graph\" #\"graph\", \"node\", set the type of classification\n",
    "len_type = \"un/cut\" #\"un/cut\", \"random\", set the shape of data used in later paths \n",
    "path_main = \"dataset_uncut.csv\" # \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "path_properties = \"dataset_properties.npz\"  # path to properties used for random \n",
    "path_mask = \"dataset_mask.npz\" # path to mask dataset used for random\n",
    "\n",
    "# batch_size = 64*2 #set the train batch size\n",
    "learning_rate = 0.01\n",
    "range_epoch = 1000 #set length of epoch\n",
    "epoch_min = 5\n",
    "epoch_interval = 5\n",
    "\n",
    "analysis = True\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "# my data        \n",
    "output = generate_output()\n",
    "torch.manual_seed(6406)\n",
    "\n",
    "train_size = int(0.6 * len(output))\n",
    "Temp_size = len(output) - train_size\n",
    "val_size = int(0.5*Temp_size)\n",
    "test_size = Temp_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "# model\n",
    "model = LitAutoEncoder().double()#.to(device)\n",
    "\n",
    "#training\n",
    "val_check_interval=len(train_loader)\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs = 100,\n",
    "#                      callbacks=[latest_checkpoint,val_checkpoint],\n",
    "#                      accelerator='gpu',\n",
    "#                      devices=1,\n",
    "#                      default_root_dir=\"saves\")\n",
    "\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 100,accelerator='gpu',devices=1,default_root_dir=\"saves\")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "684ff0db-cc7f-46fa-bc2c-4683daaabd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5718e77136a241e9970bb4ddbbbb3d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9335845708847046\n",
      "        test_loss           1.9759774208068848\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.9759774208068848, 'test_acc': 0.9335845708847046}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eb2f486d-1afb-4f46-8bb4-c3d92aaa0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc5b3df2864feb909c2856d7e1f8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.979745626449585\n",
      "        test_loss           1.9759774208068848\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.9759774208068848, 'test_acc': 0.979745626449585}]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitAutoEncoder.load_from_checkpoint(\"saves/lightning_logs/with_LRfinder_e_100/checkpoints/epoch=94-step=9300-val_loss=0.0.ckpt\").double()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aaf9352d-edc1-4803-ab08-c8a32dfa4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1496f64159014328a3c3bac5c37f7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.97833251953125\n",
      "        test_loss           1.9759774208068848\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.9759774208068848, 'test_acc': 0.97833251953125}]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitAutoEncoder.load_from_checkpoint(\"saves/lightning_logs/e_100/checkpoints/epoch=99-step=10000-val_loss=0.0.ckpt\").double()\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8ea75130-f77e-4259-857c-80865887608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c0bbac188474b63\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c0bbac188474b63\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=YOUR_LOG_DIR --host=127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3369e095-0060-4075-a70d-8a2e40272533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-290fb6a0fe5994ea\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-290fb6a0fe5994ea\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
